{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"about/","title":"ML-Notes","text":"<p>Welcome to my collection of structured notes on various topics in machine learning, mathematics, and computer science. These notes are compiled from courses, books, YouTube lectures, online forums, and personal study sessions. The goal is to create a structured and accessible resource for learning and revision across topics like mathematics, machine learning, and computer science. </p> <p>This is still a work in progress i.e. more content, corrections, and refinements are being added regularly.</p> <p>Info</p> <p>Disclaimer: These notes are intended for educational and reference purposes only. While I have done my best to ensure accuracy, there may be errors or omissions. The content may include interpretations or summaries of external materials; all credit goes to the original authors and sources. Please verify critical information independently, and feel free to report any mistakes or suggest improvements.</p>"},{"location":"cv/about/","title":"About","text":"<p>In progress...</p>"},{"location":"linear-algebra/about/","title":"Linear Algebra","text":""},{"location":"linear-algebra/about/#books","title":"Books","text":"<ol> <li>\"Linear Algebra Done Right\" by Sheldon Axler</li> <li>\"Linear Algebra and Its Applications\" by David C. Lay, Steven R. Lay, and Judi J. McDonald</li> <li>\u201cIntroduction to Linear Algebra\u201d by Gilbert Strang</li> </ol>"},{"location":"linear-algebra/about/#resources","title":"Resources","text":"Name URL Bhagwan Singh Playlist (LA) https://www.youtube.com/playlist?list=PLdM-WZokR4taLvoJPvfHwF8m0Q1K6Qvmz MKS Playlist (Matrices) https://youtube.com/playlist?list=PLhSp9OSVmeyIVQpCt2kwsC1dNVl1GwlVn&amp;si=N4hQBEKy0jhuO2u4 Identities https://dustinstansbury.github.io/theclevermachine/linear-algebra-identities Matrix Properties https://math.mit.edu/~dyatlov/54summer10/matalg.pdf"},{"location":"linear-algebra/introduction-to-vectors/lengths-and-dot-products/","title":"Lengths and Dot Products","text":""},{"location":"linear-algebra/introduction-to-vectors/lengths-and-dot-products/#lengthmagnitude","title":"Length/Magnitude","text":"<p>An important case of the dot product is when a vector is dotted with itself. In this case, we have \\(v = w\\).</p> <p>For example, if \\(v=(1,2,3)\\), then the dot product with itself is: \\(\\mathbf{v} \\cdot \\mathbf{v} = \\|\\mathbf{v}\\|^2 = 1^2 + 2^2 + 3^2 = 14\\)</p> <p>This value is known as the dot product \\(\\mathbf{v} \\cdot \\mathbf{v}\\), which also equals the squared length of the vector. There is no angle between a vector and itself; we can think of this angle as \\(0^\\circ\\), not \\(90^\\circ\\). The result is not zero because a vector is never perpendicular to itself.</p> <p>The dot product \\(\\mathbf{v} \\cdot \\mathbf{v}\\) gives the length squared of the vector.</p> <p>Definition: The length (or norm) \\(\\|\\mathbf{v}\\|\\) of a vector \\(\\mathbf{v}\\) is the square root of the dot product of the vector with itself:</p> \\[ length = \\|\\mathbf{v}\\| = \\sqrt{\\mathbf{v} \\cdot \\mathbf{v}} = \\sqrt{v_1^2 + v_2^2 + \\cdots + v_n^2} \\] <p>where \\(n\\) is the number of dimensions.</p>"},{"location":"linear-algebra/introduction-to-vectors/lengths-and-dot-products/#unit-vector","title":"Unit Vector","text":"<p>The word \"unit\" always indicates that some measurement equals one. For example:</p> <ul> <li>A unit price is the price for one item.</li> <li>A unit cube has sides of length one.</li> <li>A unit circle is a circle with radius one.</li> </ul> <p>Definition: A unit vector \\(u\\) is a vector whose length equals one. This implies: \\(u\u22c5u=1\\)</p> <p>An example in four dimensions is: \\(\\mathbf{u} = \\left( \\tfrac{1}{2}, \\tfrac{1}{2}, \\tfrac{1}{2}, \\tfrac{1}{2} \\right).\\)</p> <p>Then the dot product is: \\(\\mathbf{u} \\cdot \\mathbf{u} = \\left( \\tfrac{1}{2} \\right)^2 + \\left( \\tfrac{1}{2} \\right)^2 + \\left( \\tfrac{1}{2} \\right)^2 + \\left( \\tfrac{1}{2} \\right)^2 = \\tfrac{1}{4} + \\tfrac{1}{4} + \\tfrac{1}{4} + \\tfrac{1}{4} = 1\\).</p> <p>We obtained this unit vector by dividing the vector \\(\\mathbf{v} = (1, 1, 1, 1)\\) by its length: \\(\\|\\mathbf{v}\\| = \\sqrt{1^2 + 1^2 + 1^2 + 1^2} = \\sqrt{4} = 2\\)</p> <p>so the unit vector is: \\(\\mathbf{u} = \\frac{\\mathbf{v}}{\\|\\mathbf{v}\\|} = \\left( \\tfrac{1}{2}, \\tfrac{1}{2}, \\tfrac{1}{2}, \\tfrac{1}{2} \\right)\\).</p>"},{"location":"linear-algebra/introduction-to-vectors/lengths-and-dot-products/#inequalities","title":"Inequalities","text":"<p>No matter the angle between the vectors, the dot product of \\(\\frac{\\mathbf{v}}{\\|\\mathbf{v}\\|}\\) with \\(\\frac{\\mathbf{w}}{\\|\\mathbf{w}\\|}\\) never exceeds one.</p> <p>This fact is captured by the Cauchy\u2013Schwarz inequality, also known historically as the Schwarz inequality or the Cauchy\u2013Schwarz\u2013Bunyakovsky inequality. It was discovered independently in France, Germany, and Russia. This inequality is one of the most important in all of mathematics.</p> <p>Since \\(\\cos \\theta| \\leq 1\\), the cosine formula for the dot product implies two foundational inequalities:</p> <p>Schwarz Inequality: \\(|\\mathbf{v} \\cdot \\mathbf{w}| \\leq \\|\\mathbf{v}\\| \\, \\|\\mathbf{w}\\|\\)</p> <p>Triangle Inequality: \\(\\|\\mathbf{v} + \\mathbf{w}\\| \\leq \\|\\mathbf{v}\\| + \\|\\mathbf{w}\\|\\)</p> <p>These inequalities hold for any vectors \\(v\\) and \\(w\\) in Euclidean space. These inequalities are essential in proving orthogonality, estimating angles, and ensuring numerical stability in computations.</p>"},{"location":"linear-algebra/introduction-to-vectors/lengths-and-dot-products/#dot-product","title":"Dot Product","text":"<p>The dot product (also called scalar product) takes two vectors and gives a number (a scalar), not a vector.</p>"},{"location":"linear-algebra/introduction-to-vectors/lengths-and-dot-products/#formula","title":"Formula","text":"<p>If</p> \\[ \\mathbf{v} = \\begin{bmatrix} v_1 \\\\ v_2 \\end{bmatrix}, \\quad \\mathbf{w} = \\begin{bmatrix} w_1 \\\\ w_2 \\end{bmatrix} \\] <p>then:The dot product w \u00b7 v equals v \u00b7 w. The order of v and w makes no difference.</p> \\[ \\mathbf{v} \\cdot \\mathbf{w} = v_1 w_1 + v_2 w_2 \\] <p>In 3D:</p> \\[ \\mathbf{v} \\cdot \\mathbf{w} = v_1 w_1 + v_2 w_2 + v_3 w_3 \\]  \ud83d\udca1  The dot product $w \\cdot v$ equals $v \\cdot w$. The order of v and w makes no difference."},{"location":"linear-algebra/introduction-to-vectors/lengths-and-dot-products/#geometric-meaning","title":"Geometric Meaning","text":"<p>The dot product also equals:</p> \\[ \\mathbf{v} \\cdot \\mathbf{w} = \\|\\mathbf{v}\\| \\|\\mathbf{w}\\| \\cos(\\theta) \\] <p>where \u03b8 is the angle between the vectors.</p> <p>This tells us:</p> <ul> <li>If the dot product is positive, the angle is acute (&lt; 90\u00b0) (~same direction)</li> <li>If it is zero, the vectors are perpendicular (orthogonal)</li> <li>If it is negative, the angle is obtuse (&gt; 90\u00b0) (~opposite direction)</li> </ul>"},{"location":"linear-algebra/introduction-to-vectors/lengths-and-dot-products/#example","title":"Example","text":"<p>Let</p> \\[ \\mathbf{v} = \\begin{bmatrix} 1 \\\\ 3 \\end{bmatrix}, \\quad \\mathbf{w} = \\begin{bmatrix} 4 \\\\ -2 \\end{bmatrix} \\] <p>Then:</p> \\[ \\mathbf{v} \\cdot \\mathbf{w} = 1 \\cdot 4 + 3 \\cdot (-2) = 4 - 6 = -2 \\] <p>The result is a number: -2.</p>"},{"location":"linear-algebra/introduction-to-vectors/lengths-and-dot-products/#working","title":"Working","text":"<ul> <li>The dot product helps project one vector onto another.</li> <li>It measures how much one vector extends in the direction of another.</li> <li>Used to find angles between vectors and to project one vector onto another.</li> <li>In physics, work = force \u22c5 displacement.</li> </ul>"},{"location":"linear-algebra/introduction-to-vectors/lengths-and-dot-products/#cross-product","title":"Cross Product","text":"<p>The cross product is only defined in 3D. It takes two vectors and gives a new vector, not a number.</p>"},{"location":"linear-algebra/introduction-to-vectors/lengths-and-dot-products/#formula_1","title":"Formula","text":"<p>If</p> \\[ \\mathbf{v} = \\begin{bmatrix} v_1 \\\\ v_2 \\\\ v_3 \\end{bmatrix}, \\quad \\mathbf{w} = \\begin{bmatrix} w_1 \\\\ w_2 \\\\ w_3 \\end{bmatrix} \\] <p>then:</p> \\[ \\begin{gathered} \\mathbf{v} \\times \\mathbf{w} = \\begin{vmatrix} \\mathbf{i} &amp; \\mathbf{j} &amp; \\mathbf{k} \\\\ v_1 &amp; v_2 &amp; v_3 \\\\ w_1 &amp; w_2 &amp; w_3 \\end{vmatrix} \\\\ \\mathbf{v} \\times \\mathbf{w} = (v_2 w_3 - v_3 w_2)\\mathbf{i} - (v_3 w_1 - v_1 w_3)\\mathbf{j} + (v_1 w_2 - v_2 w_1)\\mathbf{k} \\\\ \\mathbf{v} \\times \\mathbf{w} = \\begin{bmatrix}  v_2 w_3 - v_3 w_2 \\\\ v_3 w_1 - v_1 w_3 \\\\ v_1 w_2 - v_2 w_1 \\end{bmatrix} \\end{gathered} \\] <p>This new vector is perpendicular to both v and w.</p>"},{"location":"linear-algebra/introduction-to-vectors/lengths-and-dot-products/#geometric-meaning_1","title":"Geometric Meaning","text":"<p>The length of the cross product is:</p> \\[ ||\\mathbf{v} \\times \\mathbf{w}\\| = \\|\\mathbf{v}\\| \\|\\mathbf{w}\\| \\sin(\\theta) \\] <p>This is the area of the parallelogram formed by the two vectors.</p>"},{"location":"linear-algebra/introduction-to-vectors/lengths-and-dot-products/#right-hand-rule","title":"Right-Hand Rule","text":"<p>To find the direction of the cross product:</p> <ol> <li>Point your right-hand fingers along v</li> <li>Curl toward w</li> <li>Your thumb points in the direction of \\(\\mathbf{v} \\times \\mathbf{w}\\)</li> </ol>"},{"location":"linear-algebra/introduction-to-vectors/lengths-and-dot-products/#example_1","title":"Example","text":"<p>Let</p> \\[ \\mathbf{v} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\quad \\mathbf{w} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix} \\] <p>Then:</p> \\[ \\begin{gathered} \\mathbf{v} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\quad \\mathbf{w} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix} \\\\ \\mathbf{v} \\times \\mathbf{w} = \\begin{vmatrix} \\mathbf{i} &amp; \\mathbf{j} &amp; \\mathbf{k} \\\\ 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\end{vmatrix} \\\\ \\mathbf{v} \\times \\mathbf{w} = \\mathbf{i}(0 \\cdot 0 - 0 \\cdot 1) - \\mathbf{j}(1 \\cdot 0 - 0 \\cdot 0) + \\mathbf{k}(1 \\cdot 1 - 0 \\cdot 0) \\\\ \\mathbf{v} \\times \\mathbf{w} = \\mathbf{i}(0) - \\mathbf{j}(0) + \\mathbf{k}(1) \\\\ \\mathbf{v} \\times \\mathbf{w} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix} \\end{gathered} \\] <p>The result is a vector pointing along the z-axis.</p>"},{"location":"linear-algebra/introduction-to-vectors/lengths-and-dot-products/#working_1","title":"Working","text":"<ul> <li>Produces a vector perpendicular (normal) to both input vectors.</li> <li>Direction follows the right-hand rule.</li> <li>Magnitude equals the area of the parallelogram formed by the two vectors.</li> <li>Zero vector if the vectors are parallel or one is zero.</li> <li>Useful for finding normals to planes and calculating torque.</li> </ul>"},{"location":"linear-algebra/introduction-to-vectors/matrices/","title":"Matrices","text":""},{"location":"linear-algebra/introduction-to-vectors/matrices/#properties","title":"Properties","text":"<p>Properties of matrix operations:</p> <ul> <li>Addition: If \\(A\\) and \\(B\\) are matrices of the same size \\(m\u00d7n\\), then their sum \\(A+B\\) is also an \\(m\u00d7n\\) matrix.</li> <li>Multiplication by scalars: If \\(A\\) is an \\(m\u00d7n\\) matrix and \\(c\\) is a scalar, then \\(cA\\) is an \\(m\u00d7n\\) matrix.</li> <li>Matrix multiplication: If \\(A\\) is an \\(m\u00d7n\\) matrix and \\(B\\) is an \\(n\u00d7p\\) matrix, then the product \\(AB\\) is an \\(m\u00d7p\\) matrix.</li> <li>Vectors: A vector of length \\(n\\) can be treated as an \\(n\u00d71\\) matrix. Vector addition, scalar multiplication, and matrix-vector multiplication follow the same rules as matrix operations.</li> <li>Transpose: For an \\(m\u00d7n\\) matrix \\(A\\), its transpose \\(A^T\\) is an \\(n\u00d7m\\) matrix.</li> <li>Identity matrix: \\(I_n\\) is the \\(n\u00d7n\\) identity matrix, with 1's on the diagonal and 0's elsewhere.</li> <li>Zero matrix: Denoted by 0, it is a matrix of all zeroes with appropriate size.</li> <li>Inverse: For a square matrix \\(A\\), its inverse \\(A^{-1}\\) is a matrix of the same size such that \\(AA^{-1} = A^{-1}A = I_n\\). Not all matrices have inverses; those that do are called invertible.</li> </ul> <p>Key properties (assuming scalars \\(r, s\\) and appropriately sized matrices \\(A, B, C\\)):</p> <p>Properties of matrix addition:</p> \\[ \\begin{aligned} &amp; A + B = B + A &amp;&amp; \\text{(Commutativity)} \\\\ &amp; (A + B) + C = A + (B + C) &amp;&amp; \\text{(Associativity)} \\\\ &amp; A + 0 = A, &amp;&amp; \\text{(Additive identity)} \\\\ &amp; r(A + B) = rA + rB &amp;&amp; \\text{(Distributivity of scalar over addition)} \\\\ &amp; (r + s)A = rA + sA &amp;&amp; \\text{(Distributivity of addition over scalar)} \\\\ &amp; r(sA) = (rs)A &amp;&amp; \\text{(Associativity of scalar multiplication)} \\\\ \\end{aligned} \\] <p>Properties of matrix multiplication:</p> \\[ \\begin{aligned} &amp; A(BC) = (AB)C &amp;&amp; \\text{(Associativity)} \\\\ &amp; A(B + C) = AB + AC &amp;&amp; \\text{(Left distributivity)} \\\\ &amp; (B + C)A = BA + CA &amp;&amp; \\text{(Right distributivity)} \\\\ &amp; r(AB) = (rA)B = A(rB) &amp;&amp; \\text{(Scalar multiplication compatibility)} \\\\ &amp; I_m A = A = A I_n &amp;&amp; \\text{(Identity matrix)} \\\\ \\end{aligned} \\] <p>Properties of the transpose operation:</p> \\[ \\begin{aligned} &amp; (A^T)^T = A, &amp;&amp; \\text{(Double transpose)} \\\\ &amp; (A + B)^T = A^T + B^T &amp;&amp; \\text{(Transpose of sum)} \\\\ &amp; (rA)^T = r A^T &amp;&amp; \\text{(Transpose of scalar multiple)} \\\\ &amp; (AB)^T = B^T A^T &amp;&amp; \\text{(Transpose of product reverses order)} \\\\ &amp; (I_n)^T = I_n &amp;&amp; \\text{(Transpose of identity)} \\\\ \\end{aligned} \\] <p>Properties of the inverse operation (for invertible matrices):</p> \\[ \\begin{aligned} &amp; A A^{-1} = A^{-1} A = I_n, &amp;&amp; \\text{(Inverse definition)} \\\\ &amp; (rA)^{-1} = r^{-1} A^{-1} \\quad r \\neq 0, &amp;&amp; \\text{(Inverse of scalar multiple)} \\\\ &amp; (AB)^{-1} = B^{-1} A^{-1} &amp;&amp; \\text{(Inverse of product reverses order)} \\\\ &amp; (I_n)^{-1} = I_n &amp;&amp; \\text{(Inverse of identity)} \\\\ &amp; (A^T)^{-1} = (A^{-1})^T &amp;&amp; \\text{(Transpose and inverse commute)} \\\\ &amp; (A^{-1})^{-1} = A &amp;&amp; \\text{(Inverse of inverse)} \\\\ \\end{aligned} \\] <p>Additional Properties</p> \\[ \\begin{aligned} &amp; A = A^T \\quad \\text{(Symmetric matrix)} \\\\ &amp; Q^T Q = I_n \\quad \\text{(Orthogonal matrix)} \\\\ &amp; \\mathrm{tr}(A + B) = \\mathrm{tr}(A) + \\mathrm{tr}(B) \\quad \\text{(Trace linearity)} \\\\ &amp; \\mathrm{tr}(AB) = \\mathrm{tr}(BA) \\quad \\text{(Trace cyclic property)} \\\\ &amp; \\det(AB) = \\det(A) \\det(B) \\quad \\text{(Determinant multiplicative)} \\\\ &amp; \\det(A^T) = \\det(A) \\quad \\text{(Determinant transpose)} \\\\ &amp; \\det(A^{-1}) = \\frac{1}{\\det(A)} \\quad \\text{(Determinant inverse)} \\\\ &amp; \\operatorname{rank}(A) \\leq \\min(m, n) \\quad \\text{for } A \\in \\mathbb{R}^{m \\times n} \\\\ &amp; \\operatorname{rank}(AB) \\leq \\min(\\operatorname{rank}(A), \\operatorname{rank}(B)) \\\\ &amp; \\text{If } A \\text{ is invertible and } AB = AC \\text{ then } B = C \\quad \\text{(Cancellation law)} \\\\ &amp; \\text{In general, } AB \\neq BA \\quad \\text{(Non-commutativity)} \\\\ &amp; (A + B)(A - B) = A^2 - B^2 \\quad \\text{only if } AB = BA \\quad \\text{(Difference of squares)} \\\\ &amp; \\text{If } A \\text{ is symmetric and positive definite, } \\exists \\text{ Cholesky factorization } A = LL^T \\\\ &amp; \\text{Eigenvalues of } A^T = \\text{Eigenvalues of } A \\\\ &amp; \\text{Eigenvalues of } A^{-1} = \\frac{1}{\\text{Eigenvalues of } A} \\end{aligned} \\] <p>Differences from regular number operations:</p> <ul> <li>Matrix multiplication is generally not commutative; in general, \\(AB \\neq BA\\).</li> <li>The transpose of a product reverses order: \\((AB)^T = B^T A^T\\).</li> <li>The inverse of a product reverses order: \\((AB)^{-1} = B^{-1} A^{-1}\\).</li> <li>To conclude \\(B = C\\) from \\(AB = AC\\), matrix \\(A\\) must be invertible.</li> <li>If \\(AB = 0\\), it does not imply \\(A = 0\\) or \\(B = 0\\). For example,</li> </ul> \\[ A = \\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; 0 \\end{pmatrix}, \\quad B = \\begin{pmatrix} 0 &amp; 0 \\\\ 0 &amp; 1 \\end{pmatrix}, \\quad AB = \\begin{pmatrix} 0 &amp; 0 \\\\ 0 &amp; 0 \\end{pmatrix} \\]  \ud83d\udca1  A square matrix is invertible if and only if its determinant is non-zero."},{"location":"linear-algebra/introduction-to-vectors/matrices/#types","title":"Types","text":"<p>Matrices come in many forms depending on their size, elements, and special properties. Below is a detailed overview of the most common types of matrices used in linear algebra.</p>"},{"location":"linear-algebra/introduction-to-vectors/matrices/#1-square-matrix","title":"1. Square Matrix","text":"<p>A matrix with the same number of rows and columns.</p> \\[ A = \\begin{bmatrix} a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1n} \\\\ a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{n1} &amp; a_{n2} &amp; \\cdots &amp; a_{nn} \\end{bmatrix} \\] <p>Example: \\(\\begin{bmatrix} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\\\ 7 &amp; 8 &amp; 9 \\end{bmatrix}\\)</p>"},{"location":"linear-algebra/introduction-to-vectors/matrices/#2-rectangular-matrix","title":"2. Rectangular Matrix","text":"<p>A matrix where the number of rows is not equal to the number of columns.</p> \\[ B = \\begin{bmatrix} b_{11} &amp; b_{12} &amp; \\cdots &amp; b_{1p} \\\\ b_{21} &amp; b_{22} &amp; \\cdots &amp; b_{2p} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ b_{m1} &amp; b_{m2} &amp; \\cdots &amp; b_{mp} \\end{bmatrix} \\] <p>with \\(m\\neq n\\) .</p>"},{"location":"linear-algebra/introduction-to-vectors/matrices/#3-row-matrix","title":"3. Row Matrix","text":"<p>A matrix with only one row \\(1 \\times n\\).</p> \\[ R = \\begin{bmatrix} r_1 &amp; r_2 &amp; \\cdots &amp; r_n \\end{bmatrix} \\]"},{"location":"linear-algebra/introduction-to-vectors/matrices/#4-column-matrix","title":"4. Column Matrix","text":"<p>A matrix with only one column \\(m \\times 1\\).</p> \\[ C = \\begin{bmatrix} c_1 \\\\ c_2 \\\\ \\vdots \\\\ c_m \\end{bmatrix} \\]"},{"location":"linear-algebra/introduction-to-vectors/matrices/#5-zero-matrix-null-matrix","title":"5. Zero Matrix (Null Matrix)","text":"<p>A matrix in which all elements are zero.</p> \\[ O = \\begin{bmatrix} 0 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; 0 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; 0 \\end{bmatrix}\u00a0 \\]"},{"location":"linear-algebra/introduction-to-vectors/matrices/#6-identity-matrix","title":"6. Identity Matrix","text":"<p>A square matrix with ones on the main diagonal and zeros elsewhere.</p> \\[ I_n = \\begin{bmatrix} 1 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; 1 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; 1 \\end{bmatrix} \\]"},{"location":"linear-algebra/introduction-to-vectors/matrices/#7-diagonal-matrix","title":"7. Diagonal Matrix","text":"<p>A square matrix where all off-diagonal elements are zero.</p> \\[ D = \\begin{bmatrix} d_1 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; d_2 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; d_n \\end{bmatrix} \\]"},{"location":"linear-algebra/introduction-to-vectors/matrices/#8-scalar-matrix","title":"8. Scalar Matrix","text":"<p>A diagonal matrix where all diagonal entries are equal.</p> \\[ S = \\lambda I_n = \\begin{bmatrix} \\lambda &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\lambda &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; \\lambda \\end{bmatrix} \\]"},{"location":"linear-algebra/introduction-to-vectors/matrices/#9-symmetric-matrix","title":"9. Symmetric Matrix","text":"<p>A square matrix that is equal to its transpose: \\(A = A^T\\)</p>"},{"location":"linear-algebra/introduction-to-vectors/matrices/#10-skew-symmetric-antisymmetric-matrix","title":"10. Skew-Symmetric (Antisymmetric) Matrix","text":"<p>A square matrix whose transpose equals its negative: \\(A^T = -A\\)</p>"},{"location":"linear-algebra/introduction-to-vectors/matrices/#11-orthogonal-matrix","title":"11. Orthogonal Matrix","text":"<p>A square matrix \\(Q\\) with real entries whose transpose is its inverse: </p> \\[ \\begin{gathered} Q^T = Q^{-1} \\\\\\text{OR}\\\\ Q^T Q = Q Q^T = I_n \\end{gathered} \\] <p>Example: A  rotation matrix \\(Q = \\begin{bmatrix} \\cos \\theta &amp; -\\sin \\theta \\\\ \\sin \\theta &amp; \\cos \\theta \\end{bmatrix}\\)</p>"},{"location":"linear-algebra/introduction-to-vectors/matrices/#12-singular-matrix","title":"12. Singular Matrix","text":"<p>A square matrix that does not have an inverse. Its determinant is zero: \\(\\det(A) = 0\\)</p>"},{"location":"linear-algebra/introduction-to-vectors/matrices/#13-invertible-nonsingular-matrix","title":"13. Invertible (Nonsingular) Matrix","text":"<p>A square matrix \\(A\\) that has an inverse: \\(A^{-1} = A^{-1} A = I_n\\)</p>"},{"location":"linear-algebra/introduction-to-vectors/matrices/#14-positive-definite-matrix","title":"14. Positive Definite Matrix","text":"<p>A symmetric matrix \\(A\\) such that for all nonzero vectors \\(x\\), \\(x^T A x &gt; 0\\)</p> <p>Its eigenvalues are strictly positive. If the quadratic form is strictly greater than zero for all non-zero vectors x, then the matrix is called positive definite.</p>"},{"location":"linear-algebra/introduction-to-vectors/matrices/#15-positive-semi-definite-psd-matrix","title":"15. Positive Semi-Definite (PSD) Matrix","text":"<p>A symmetric matrix \\(A\\) is positive semi-definite if for all vectors \\(x\\), \\(x^T A x \\geq 0\\)</p> <p>Its eigenvalues are non-negative. This means the quadratic form is never negative, but it can be zero for some nonzero \\(x\\).</p>"},{"location":"linear-algebra/introduction-to-vectors/matrices/#16-hermitian-matrix","title":"16. Hermitian Matrix","text":"<p>A square matrix \\(A\\) with complex entries is Hermitian if it equals its own conjugate transpose: \\(A = A^H = \\overline{A}^T\\)</p> <p>This means \\(A_{ij} = \\overline{A_{ji}}\\) for all \\(i,j\\).</p> <p>Example: \\(A = \\begin{bmatrix} 2 &amp; 2 + i \\\\ 2 - i &amp; 3 \\end{bmatrix}\\)Here, \\(A = A^H\\).</p>"},{"location":"linear-algebra/introduction-to-vectors/matrices/#17-skew-hermitian-matrix","title":"17. Skew-Hermitian Matrix","text":"<p>A square matrix A is skew-Hermitian if it satisfies: \\(A = -A^H = -\\overline{A}^T\\)(i.e. if and only if it is equal to the negative of its conjugate matrix).</p> <p>This means \\(A_{ij} = -\\overline{A_{ji}}\\).</p> <p>Example: \\(A = \\begin{bmatrix} 0 &amp; 2 + i \\\\ -2 + i &amp; 0 \\end{bmatrix}\\)Here, \\(A = -A^H\\).</p>"},{"location":"linear-algebra/introduction-to-vectors/matrices/#18-idempotent-matrix","title":"18. Idempotent Matrix","text":"<p>A square matrix A is idempotent if: \\(A^2 = A\\) or \\(A^n\u00a0= A\\), for every \\(n \u2265 2\\)</p> <p>This means multiplying the matrix (not element-wise) by itself returns the same matrix.</p>"},{"location":"linear-algebra/introduction-to-vectors/matrices/#19-nilpotent-matrix","title":"19. Nilpotent Matrix","text":"<p>A square matrix \\(A\\) of order \\(n\\) is nilpotent if there exists an integer \\(k \\leq n\\) such that: \\(A^k = 0\\)where \\(0\\) is the zero matrix.</p>"},{"location":"linear-algebra/introduction-to-vectors/matrices/#20-involutory-matrix","title":"20. Involutory Matrix","text":"<p>A square matrix \\(A\\) is involutory if it is its own inverse: \\(A^{-1} = A\\)</p> <p>For example, an identity matrix is involutory as it is equal to its\u00a0inverse.</p>"},{"location":"linear-algebra/introduction-to-vectors/matrices/#references","title":"References","text":"<ol> <li>https://math.mit.edu/~dyatlov/54summer10/matalg.pdf</li> </ol>"},{"location":"linear-algebra/introduction-to-vectors/planes/","title":"Planes","text":""},{"location":"linear-algebra/introduction-to-vectors/planes/#what-is-a-plane","title":"What is a Plane?","text":"<p>A plane is a flat, two-dimensional surface that extends infinitely in 3D space. Think of it like an endless sheet of paper floating in space, it has length and width, but no thickness.</p>"},{"location":"linear-algebra/introduction-to-vectors/planes/#how-do-we-define-a-plane","title":"How Do We Define a Plane?","text":"<p>A plane in 3D space can be defined in two main ways:</p>"},{"location":"linear-algebra/introduction-to-vectors/planes/#1-using-a-point-and-a-normal-vector-general","title":"1. Using a Point and a Normal Vector (General)","text":"<p>The general equation of a plane is:</p> \\[ \\begin{gathered} ax+by+cz=d \\\\ \\vec{n}\\cdot(\\vec{r}-\\vec{r_0}) \\end{gathered} \\] <p>Here:</p> <ul> <li>\\(\\vec{n}=\u27e8a,b,c\u27e9\\) is the normal vector i.e. a vector that is perpendicular to the plane and <code>a,b,c</code> are the components of the vectors.</li> <li>\\(\\vec{r} = \u27e8x,y,z\u27e9\\) is a variable point on the plane.</li> <li>\\(\\vec{r_0} = \u27e8x_0,y_0,z_0\u27e9\\) is a known point on the plane.</li> <li><code>d</code> is a constant that shifts the plane away from the origin along the direction of the normal vector.</li> </ul> <p>This form tells us: any point <code>(x, y, z)</code> that satisfies the equation lies on the plane.</p> <p>Example:</p> \\[ \\begin{gathered} \\vec{n}\\cdot(\\vec{r}-\\vec{r_0}) = \\langle 4, 5, 6 \\rangle \\cdot \\langle x - 1, y - 2, z - 3 \\rangle \\\\ 4(x - 1) + 5(y - 2) + 6(z - 3) = 0 \\\\ 4x + 5y + 6z = 32 \\end{gathered} \\] <p>This describes a plane in 3D space with normal vector <code>(2,3,5)</code>.</p>"},{"location":"linear-algebra/introduction-to-vectors/planes/#2-using-two-vectors-parametric-form","title":"2. Using Two Vectors (Parametric Form)","text":"<p>A plane can also be defined using two linearly independent vectors that lie in the plane. Generally used for visualizations, sampling etc. If \\(v\\) and \\(w\\) are such vectors, then all points on the plane can be described by:</p> <p>To define a plane using vectors, you need:</p> <ul> <li>A point \\(\\mathbf{p} = (x_0, y_0, z_0)\\) that lies on the plane</li> <li>Two linearly independent vectors \\(v, w\\) that lie in the plane</li> </ul> <p>Then the parametric equation of the plane is:</p> \\[ r(s,t)=p+sv+tw \\] <p>Where:</p> <ul> <li>\\(s\\) and \\(t\\) are real numbers (parameters)</li> <li>\\(r(s,t)\\) gives all points on the plane</li> </ul>"},{"location":"linear-algebra/introduction-to-vectors/planes/#example","title":"Example:","text":"<p>Let:</p> <ul> <li>Point on the plane: \\(p=(1,0,0)\\)</li> <li>Direction vectors: \\(v=(2,2,0),  w=(0,2,1)\\)</li> </ul> <p>Then the plane is:</p> \\[ r(s,t)=(1,0,0)+s(2,2,0)+t(0,2,1) \\] <p>Expanding:</p> \\[ r(s,t)=(1+2s,2s+2t,t) \\] <p>This gives all points on the plane by varying \\(s\\) and \\(t\\).</p>"},{"location":"linear-algebra/introduction-to-vectors/planes/#why-are-planes-important","title":"Why Are Planes Important?","text":"<p>Planes are fundamental in geometry, linear algebra, computer graphics, and physics. They help:</p> <ul> <li>Visualize and solve systems of linear equations</li> <li>Define surfaces, collisions, or boundaries</li> <li>Represent flat objects like walls, screens, or layers in 3D models</li> </ul>"},{"location":"linear-algebra/introduction-to-vectors/planes/#references","title":"References","text":"<ol> <li>https://www.youtube.com/watch?v=HjJ140TYbXQ</li> </ol>"},{"location":"linear-algebra/introduction-to-vectors/preface/","title":"Introduction to Vectors","text":""},{"location":"linear-algebra/introduction-to-vectors/preface/#what","title":"What","text":"<p>A vector is a mathematical object that has both magnitude (size) and direction. It is often represented as an arrow in space or as an ordered list of numbers.</p> <p>Example: In 2D, the vector v = [3, 4] represents a movement 3 units along the x-axis and 4 units along the y-axis.</p>"},{"location":"linear-algebra/introduction-to-vectors/preface/#how","title":"How","text":"<p>Vectors are typically written as coordinate pairs or triplets (like [x, y] or [x, y, z]) depending on the number of dimensions. You can perform operations such as:</p> <ul> <li>Addition: [1, 2] + [3, 4] = [4, 6]</li> <li>Scalar multiplication: 2 \u00d7 [3, 4] = [6, 8]</li> <li>Dot product: [1, 2] \u2022 [3, 4] = 1\u00d73 + 2\u00d74 = 11</li> </ul> <p>These operations are useful for combining or comparing vectors.</p>"},{"location":"linear-algebra/introduction-to-vectors/preface/#why","title":"Why","text":"<p>Vectors are fundamental in fields like physics, computer graphics, and machine learning because they model quantities with direction and magnitude. For instance:</p> <ul> <li>In physics, a force is a vector.</li> <li>In computer graphics, vectors describe movement and lighting.</li> <li>In machine learning, data points and weights are represented as vectors for efficient computation.</li> </ul>"},{"location":"linear-algebra/introduction-to-vectors/vectors-and-linear-combinations/","title":"Vectors and Linear Combinations","text":""},{"location":"linear-algebra/introduction-to-vectors/vectors-and-linear-combinations/#what-is-a-linear-combination","title":"What is a Linear Combination?","text":"<p>A linear combination of two vectors means multiplying them by numbers (called scalars) and adding the results. For example:</p> \\[ 3v+5w \\] <p>This is a typical linear combination of the vectors v and w.</p> <p>If</p> \\[ \\mathbf{v} = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}, \\quad \\mathbf{w} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} \\] <p>then</p> \\[ 3\\mathbf{v} + 5\\mathbf{w} = 3\\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} + 5\\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 3 \\\\ 6 \\end{bmatrix} + \\begin{bmatrix} 0 \\\\ 5 \\end{bmatrix} = \\begin{bmatrix} 3 \\\\ 11 \\end{bmatrix} \\] <p>This means we move 3 units across (in x direction) and 11 units up (in y direction).</p>"},{"location":"linear-algebra/introduction-to-vectors/vectors-and-linear-combinations/#visualizing-vectors","title":"Visualizing Vectors","text":"<p>A vector like \\(\\begin{bmatrix} 2 \\\\ 3 \\end{bmatrix}\\)goes 2 steps in x and 3 steps in y. It points to the location (2, 3) in the xy-plane.</p> <p>All combinations like \\(\\mathbf{v} + d\\mathbf{w}\\) (fill in every point in the plane) if v and w point in different directions. They cover the whole 2D space.</p> <p>In 3D space, a combination like </p> \\[ c\\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix} + d\\begin{bmatrix} 4 \\\\ 5 \\\\ 6 \\end{bmatrix} \\] <p>fills a plane in xyz space (but not the full space unless we add a third independent vector).</p>"},{"location":"linear-algebra/introduction-to-vectors/vectors-and-linear-combinations/#vector-operations","title":"Vector Operations","text":"<p>A vector has multiple components, like:</p> \\[ \\mathbf{v} = \\begin{bmatrix} v_1 \\\\ v_2 \\end{bmatrix} \\] <p>We treat v as a single object, not two separate numbers.</p>"},{"location":"linear-algebra/introduction-to-vectors/vectors-and-linear-combinations/#vector-addition","title":"Vector Addition","text":"<p>Add corresponding parts:</p> \\[ \\mathbf{v} = \\begin{bmatrix} v_1 \\\\ v_2 \\end{bmatrix}, \\quad \\mathbf{w} = \\begin{bmatrix} w_1 \\\\ w_2 \\end{bmatrix} \\] <p>Then:</p> \\[ \\mathbf{v} + \\mathbf{w} = \\begin{bmatrix} v_1 + w_1 \\\\ v_2 + w_2 \\end{bmatrix} \\]"},{"location":"linear-algebra/introduction-to-vectors/vectors-and-linear-combinations/#vector-subtraction","title":"Vector Subtraction","text":"<p>Same idea:</p> \\[ \\mathbf{v} - \\mathbf{w} = \\begin{bmatrix} v_1 - w_1 \\\\ v_2 - w_2 \\end{bmatrix} \\]"},{"location":"linear-algebra/introduction-to-vectors/vectors-and-linear-combinations/#scalar-multiplication","title":"Scalar Multiplication","text":"<p>Multiply every part of a vector by a number:</p> \\[ \\mathbf{v} = 2\\begin{bmatrix} v_1 \\\\ v_2 \\end{bmatrix} = \\begin{bmatrix} 2v_1 \\\\ 2v_2 \\end{bmatrix}, \\quad -\\mathbf{v} = \\begin{bmatrix} -v_1 \\\\ -v_2 \\end{bmatrix} \\] <p>A scalar is just a number like 2 or -1.</p> <p>The sum of a vector and its negative gives the zero vector:</p> \\[ \\mathbf{v} + (-\\mathbf{v}) = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} \\]  \ud83d\udca1  This is different from the number zero! It is a vector with all components equal to zero."},{"location":"linear-algebra/introduction-to-vectors/vectors-and-linear-combinations/#linear-combinations-in-practice","title":"Linear Combinations in Practice","text":"<p>A linear combination means: \\(c\\mathbf{v} + d\\mathbf{w}\\)</p> <p>You can also get:</p> <ul> <li>Sum: \\(1v+1w\\)</li> <li>Difference: \\(1v\u22121w\\)</li> <li>Zero vector: \\(0v+0w\\)</li> <li>Vector \\(cv\\) in the direction of \\(v\\): \\(cv+0wc\\)</li> </ul> <p>All of these are valid linear combinations.</p>"},{"location":"linear-algebra/introduction-to-vectors/vectors-and-linear-combinations/#representing-vectors","title":"Representing Vectors","text":"<p>There are three main ways to describe a vector v:</p> <ul> <li>By its components: (4, 2)</li> <li>As an arrow from (0, 0) to (4, 2)</li> <li>As a point at (4, 2) in the plane</li> </ul> <p>You can visualize v + w as going along v, then along w, or directly along the diagonal from start to finish.</p>"},{"location":"linear-algebra/introduction-to-vectors/vectors-and-linear-combinations/#vectors-in-3d","title":"Vectors in 3D","text":"<p>A 3D vector looks like: \\(\\mathbf{v} = \\begin{bmatrix} v_1 \\\\ v_2 \\\\ v_3 \\end{bmatrix}\\)This points from \\((0, 0, 0)\\) to \\((v\u2081, v\u2082, v\u2083)\\) in 3D space.</p> <p>Writing \\(\\mathbf{v} = (1, 1, -1)\\) is a shortcut for the column vector\\(\\begin{bmatrix} 1 \\\\ 1 \\\\ -1 \\end{bmatrix}\\)It is not a row vector which is the transpose.</p> <p>Addition in 3D works the same way: \\(\\mathbf{v} + \\mathbf{w} = \\begin{bmatrix} v_1 + w_1 \\\\ v_2 + w_2 \\\\ v_3 + w_3 \\end{bmatrix}\\)</p>"},{"location":"linear-algebra/solving-linear-equations/methods/","title":"Methods","text":""},{"location":"linear-algebra/solving-linear-equations/methods/#matrix-methods","title":"Matrix methods","text":"<p>For general \\(m \\times n\\) systems</p> <ol> <li> <p>Matrix form: \\(A\\vec{x} = \\vec{b}\\)</p> <p>Where:</p> <ul> <li>\\(A\\) is an \\(m \\times n\\) matrix of coefficients</li> <li>\\(\\vec{x}\\) is an \\(n \\times 1\\) vector of variables</li> <li>\\(\\vec{b}\\) is an \\(m \\times 1\\) output vector</li> <li>Gaussian elimination / Row reduction</li> </ul> <p>Convert to row echelon form, solve via back-substitution</p> <ol> <li>Inverse method (only if \\(A\\) is square and invertible): \\(\\vec{x} = A^{-1} \\vec{b}\\)</li> <li>Least squares (for overdetermined):</li> </ol> <p>Solve: \\(A\\vec{x} = A^T\\vec{b}\\)</p> </li> </ol>  \ud83d\udca1  Row echelon form (REF) is\u00a0a matrix format where all zero rows are at the bottom (not always), and the first non-zero entry (leading entry) in each non-zero row is to the right of the leading entry in the row above it."},{"location":"linear-algebra/solving-linear-equations/methods/#substitution-elimination-small-systems","title":"Substitution / Elimination (Small Systems)","text":"<p>For small systems (like \\(2 \\times 2\\) or \\(3 \\times 3\\))</p> <ul> <li>Example 1:</li> </ul> \\[ \\begin{align*}\\text{Given:} \\quad &amp; \\begin{cases}x + y = 5 \\quad \\text{(1)} \\\\2x - y = 4 \\quad \\text{(2)}\\end{cases} \\\\[5pt] \\text{From (1):} \\quad &amp; y = 5 - x \\\\[5pt] \\text{Substitute into (2):} \\quad &amp; 2x - (5 - x) = 4 \\\\&amp; 2x - 5 + x = 4 \\\\&amp; 3x = 9 \\\\&amp; x = 3 \\\\[5pt] \\text{Substitute back:} \\quad &amp; y = 5 - 3 = 2 \\\\[5pt] \\text{Solution:} \\quad &amp; x = 3,\\quad y = 2\\end{align*} \\] <ul> <li> <p>Example 2</p> \\[ \\begin{align*}\\text{Given:} \\quad &amp; \\begin{cases}2x + 3y = 8 \\quad \\text{(1)} \\\\4x - 3y = 4 \\quad \\text{(2)}\\end{cases} \\\\[5pt] \\text{Add (1) and (2):} \\quad &amp; (2x + 3y) + (4x - 3y) = 8 + 4 \\\\&amp; 6x = 12 \\\\&amp; x = 2 \\\\[5pt] \\text{Substitute into (1):} \\quad &amp; 2(2) + 3y = 8 \\\\&amp; 4 + 3y = 8 \\\\&amp; 3y = 4 \\\\&amp; y = \\frac{4}{3} \\\\[5pt] \\text{Solution:} \\quad &amp; x = 2,\\quad y = \\frac{4}{3}\\end{align*} \\] </li> </ul>"},{"location":"linear-algebra/solving-linear-equations/methods/#gaussian-elimination-ref","title":"Gaussian Elimination (REF)","text":"<p>To solve a system of linear equations by transforming the system into an upper triangular form (all zeros below the main diagonal), and then using back-substitution to find the values of variables. Row echelon form is useful because it simplifies solving linear equations using back-substitution.</p> <p>A matrix is in row echelon form (REF) if:</p> <ul> <li>The first non-zero element in each row (called the leading entry) can be 1.</li> <li>Each leading entry is to the right of the leading entry in the row above.</li> <li>Any rows with all zeros are at the bottom of the matrix.</li> </ul> <p>A matrix is in reduced row echelon form (RREF) if:</p> <ul> <li>It meets all the conditions of row echelon form.</li> <li>Each leading entry must be 1 and is the only non-zero number in its column.</li> <li>No back substitution required. However, it is more expensive.</li> </ul>"},{"location":"linear-algebra/solving-linear-equations/methods/#process","title":"Process","text":"<ol> <li>Start with the system of equations in standard form.</li> <li>Form the augmented matrix \\([A|b]\\), where \\(A\\) is the coefficient matrix and \\(b\\) is the right-hand side.</li> <li>Forward Elimination:<ul> <li>Use row operations to create zeros below the pivots (leading coefficients) in each column.</li> <li>The goal is to get an upper triangular matrix \\(U\\) such that \\(Ux = c.\\)</li> </ul> </li> <li>Back Substitution:<ul> <li>Once in triangular form, solve the last equation for the last variable.</li> <li>Substitute upward into previous equations to find remaining variables.</li> </ul> </li> </ol>"},{"location":"linear-algebra/solving-linear-equations/methods/#row-operations-used","title":"Row Operations Used","text":"<ul> <li>Swap two rows</li> <li>Multiply a row by a nonzero scalar</li> <li>Add or subtract a multiple of one row to another</li> <li>Column operations are not used.</li> </ul> <p>Example:</p> <p>$$ \\begin{align*} \\text{Given:} \\quad &amp; \\begin{cases} 2x + 4y - 2z = 2 \\ 4x + 9y - 3z = 8 \\ -2x - 3y + 7z = 10 \\end{cases} \\[10pt]</p> <p>\\text{Step 1:} \\quad &amp; \\text{Row}_2 \\leftarrow \\text{Row}_2 - 2 \\times \\text{Row}_1 \\ &amp; \\Rightarrow y + z = 4 \\[5pt]</p> <p>\\text{Step 2:} \\quad &amp; \\text{Row}_3 \\leftarrow \\text{Row}_3 + \\text{Row}_1 \\ &amp; \\Rightarrow y + 5z = 12 \\[5pt]</p> <p>\\text{Step 3:} \\quad &amp; \\text{Row}_3 \\leftarrow \\text{Row}_3 - \\text{Row}_2 \\ &amp; \\Rightarrow 4z = 8 \\Rightarrow z = 2 \\[10pt]</p> <p>\\text{Echelon Matrix:} \\quad &amp; \\left[ \\begin{array}{ccc|c} \\mathbf{x} &amp; \\mathbf{y} &amp; \\mathbf{z} &amp; \\mathbf{b} \\ 2 &amp; 4 &amp; -2 &amp; 2 \\ 0 &amp; 1 &amp; 1  &amp; 4 \\ 0 &amp; 0 &amp; 4  &amp; 8 \\end{array} \\right] \\[10pt]</p> <p>\\text{Back Substitution:} \\quad &amp; y + z = 4 \\Rightarrow y = 2 \\ &amp; 2x + 4y - 2z = 2 \\Rightarrow 2x + 8 - 4 = 2 \\Rightarrow x = -1 \\[10pt]</p> <p>\\text{Solution:} \\quad &amp; (x, y, z) = (-1, 2, 2) \\end{align*} $$</p>"},{"location":"linear-algebra/solving-linear-equations/methods/#gauss-jordan-elimination-matrix-inversion-rref","title":"Gauss-Jordan Elimination (Matrix Inversion, RREF)","text":"<p>This method goes further to reduce the matrix to reduced row echelon form (RREF) where leading entries are 1 and the only nonzero entries in their columns, which directly gives the solution or the inverse matrix without back substitution.</p> <ul> <li>Write the matrix \\(A\\) alongside the identity matrix \\(I\\) as an augmented matrix \\([A | I]\\).</li> <li>Use row operations (swap, multiply row by scalar, add/subtract multiples of rows) to transform the left part \\(A\\) into the identity matrix.</li> <li>When the left side becomes \\(I\\), the right side of the augmented matrix will be \\(A^{-1}\\), the inverse of \\(A\\).</li> <li>If the left side cannot be reduced to \\(I\\), then \\(A\\) is not invertible which can be checked beforehand by calculating \\(det(A)\u22600.\\)</li> </ul> \\[ \\begin{align*} &amp; \\text{Start with augmented matrix } [A | I]: \\\\ &amp; \\quad \\left[ \\begin{array}{ccc|ccc} 2 &amp; 4 &amp; -2 &amp; 1 &amp; 0 &amp; 0 \\\\ 4 &amp; 9 &amp; -3 &amp; 0 &amp; 1 &amp; 0 \\\\ -2 &amp; -3 &amp; 7 &amp; 0 &amp; 0 &amp; 1 \\end{array} \\right] \\\\[12pt] &amp; \\text{Step 1: } R_1 \\leftarrow \\frac{1}{2} R_1 \\\\ &amp; \\quad \\Rightarrow \\quad \\left[ \\begin{array}{ccc|ccc} 1 &amp; 2 &amp; -1 &amp; \\frac{1}{2} &amp; 0 &amp; 0 \\\\ 4 &amp; 9 &amp; -3 &amp; 0 &amp; 1 &amp; 0 \\\\ -2 &amp; -3 &amp; 7 &amp; 0 &amp; 0 &amp; 1 \\end{array} \\right] \\\\[12pt] &amp; \\text{Step 2: Eliminate below } R_1: \\quad R_2 \\leftarrow R_2 - 4R_1, \\quad R_3 \\leftarrow R_3 + 2R_1 \\\\ &amp; \\quad \\Rightarrow \\quad \\left[ \\begin{array}{ccc|ccc} 1 &amp; 2 &amp; -1 &amp; \\frac{1}{2} &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; -2 &amp; 1 &amp; 0 \\\\ 0 &amp; 1 &amp; 5 &amp; 1 &amp; 0 &amp; 1 \\end{array} \\right] \\\\[12pt] &amp; \\text{Step 3: } R_3 \\leftarrow R_3 - R_2 \\\\ &amp; \\quad \\Rightarrow \\quad \\left[ \\begin{array}{ccc|ccc} 1 &amp; 2 &amp; -1 &amp; \\frac{1}{2} &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; -2 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 4 &amp; 3 &amp; -1 &amp; 1 \\end{array} \\right] \\\\[12pt] &amp; \\text{Step 4: } R_3 \\leftarrow \\frac{1}{4} R_3 \\\\ &amp; \\quad \\Rightarrow \\quad \\left[ \\begin{array}{ccc|ccc} 1 &amp; 2 &amp; -1 &amp; \\frac{1}{2} &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; -2 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; \\frac{3}{4} &amp; -\\frac{1}{4} &amp; \\frac{1}{4} \\end{array} \\right] \\\\[12pt] &amp; \\text{Step 5: Eliminate above } R_3: \\quad R_2 \\leftarrow R_2 - R_3, \\quad R_1 \\leftarrow R_1 + R_3 \\\\ &amp; \\quad \\Rightarrow \\quad \\left[ \\begin{array}{ccc|ccc} 1 &amp; 2 &amp; 0 &amp; \\frac{5}{4} &amp; -\\frac{1}{4} &amp; \\frac{1}{4} \\\\ 0 &amp; 1 &amp; 0 &amp; -\\frac{11}{4} &amp; \\frac{5}{4} &amp; -\\frac{1}{4} \\\\ 0 &amp; 0 &amp; 1 &amp; \\frac{3}{4} &amp; -\\frac{1}{4} &amp; \\frac{1}{4} \\end{array} \\right] \\\\[12pt] &amp; \\text{Step 6: Eliminate } 2 \\text{ in } R_1, \\text{ column } 2: \\quad R_1 \\leftarrow R_1 - 2 R_2 \\\\ &amp; \\quad \\Rightarrow \\quad \\left[ \\begin{array}{ccc|ccc} 1 &amp; 0 &amp; 0 &amp; \\frac{27}{4} &amp; -\\frac{11}{4} &amp; \\frac{3}{4} \\\\ 0 &amp; 1 &amp; 0 &amp; -\\frac{11}{4} &amp; \\frac{5}{4} &amp; -\\frac{1}{4} \\\\ 0 &amp; 0 &amp; 1 &amp; \\frac{3}{4} &amp; -\\frac{1}{4} &amp; \\frac{1}{4} \\end{array} \\right] \\\\[12pt] &amp; \\text{So, inverse matrix } A^{-1} = \\begin{bmatrix} \\frac{27}{4} &amp; -\\frac{11}{4} &amp; \\frac{3}{4} \\\\ -\\frac{11}{4} &amp; \\frac{5}{4} &amp; -\\frac{1}{4} \\\\ \\frac{3}{4} &amp; -\\frac{1}{4} &amp; \\frac{1}{4} \\end{bmatrix} \\\\[12pt] &amp; \\text{Multiply } A^{-1} \\text{ by } b = \\begin{bmatrix} 2 \\\\ 8 \\\\ 10 \\end{bmatrix}: \\quad x = A^{-1} b =  \\begin{bmatrix} -1 \\\\ 2 \\\\ 2 \\end{bmatrix} \\end{align*} \\]"},{"location":"linear-algebra/solving-linear-equations/methods/#solution-types-2d","title":"Solution Types (2D)","text":"<ul> <li>Unique solution: consistent and independent i.e. the lines intersect<ul> <li>Example: \\(y = 5 - x\\),  \\(y = x - 1\\) cross at \\((3, 2)\\)</li> </ul> </li> <li>Infinite solutions: consistent but dependent i.e. if the two equations are actually the same.<ul> <li>Example: \\(2y = 6x + 8\\) and \\(y = 3x + 4\\)</li> </ul> </li> <li>No solution: inconsistent system i.e. if the lines are parallel<ul> <li>Example \\(y = x + 3\\) and \\(y = x - 1\\)</li> </ul> </li> </ul>"},{"location":"linear-algebra/solving-linear-equations/methods/#solution-types-3d","title":"Solution Types (3D)","text":"<p>1. Unique solution</p> <ul> <li>Algebraic: System is consistent and all equations are independent.</li> <li>Geometric (3 variables): Three planes intersect at exactly one point.</li> <li> <p>Example:</p> \\[ \\begin{cases} x + y + z = 6 \\\\ 2x - y + z = 3 \\\\ x + 2y - z = 4 \\end{cases} \\] </li> <li> <p>Interpretation:</p> <p>The three planes meet at a single point, giving exactly one solution \\((x, y, z)\\).</p> </li> <li> <p>Infinite solutions</p> </li> <li> <p>Algebraic: System is consistent but dependent (some equations are multiples or linear combinations of others).</p> </li> <li>Geometric (3 variables):<ul> <li>The planes intersect along a line (all points on the line satisfy the system), or</li> <li>All three planes coincide (overlap completely).</li> </ul> </li> <li> <p>Example:</p> \\[ \\begin{cases} x + y + z = 3 \\\\ 2x + 2y + 2z = 6 \\\\ x - y = 1 \\end{cases} \\] </li> <li> <p>Interpretation:</p> <p>The first two equations represent the same plane (second is just twice the first).</p> <p>The third plane intersects this plane in a line, so infinitely many points satisfy all three.</p> </li> <li> <p>No solution</p> </li> <li> <p>Algebraic: System is inconsistent (contradictory equations).</p> </li> <li>Geometric (3 variables):<ul> <li>The planes do not all intersect in a common point or line.</li> <li>For example, two planes might be parallel but distinct, so no common intersection.</li> </ul> </li> <li> <p>Example:</p> \\[ \\begin{cases} x + y + z = 4 \\\\ 2x + 2y + 2z = 10 \\\\ x - y = 1 \\end{cases} \\] </li> <li> <p>Interpretation:</p> <p>The second equation contradicts the first (if multiplied by 2, the right side should be 8, not 10). </p> <p>So no point exists that satisfies all three equations simultaneously.</p> </li> </ul>"},{"location":"linear-algebra/solving-linear-equations/methods/#summary-of-methods-others","title":"Summary of Methods  (Others)","text":"<ol> <li>Gaussian Elimination \u2013 Row reduction to row echelon form.</li> <li>Gauss-Jordan Elimination \u2013 Row reduction to reduced row echelon form.</li> <li>Matrix Inversion \u2013 If \\(A\\mathbf{x} = \\mathbf{b}\\), then \\(\\mathbf{x} = A^{-1}\\mathbf{b}\\) (if \\(A\\) is invertible).</li> <li>Cramer's Rule \u2013 Uses determinants, applicable only when \\(A\\) is square and \\(\\det(A) \\neq 0\\).</li> <li>LU Decomposition \u2013 Factor \\(A = LU\\), then solve via forward and backward substitution.</li> <li>Cholesky Decomposition \u2013 For symmetric positive-definite \\(A\\), factor as \\(A = LL^T\\).</li> <li>QR Decomposition \u2013 Factor \\(A = QR\\), then solve  \\(Rx = Q^Tb\\).</li> <li>Singular Value Decomposition (SVD) \u2013 For general or ill-conditioned systems.</li> <li>Iterative Methods \u2013 e.g., Jacobi, Gauss-Seidel, Conjugate Gradient (for large sparse systems).</li> </ol>"},{"location":"linear-algebra/solving-linear-equations/methods/#references","title":"References","text":"<ol> <li>https://www.youtube.com/watch?v=eDb6iugi6Uk</li> <li>https://www.youtube.com/watch?v=hu6B1d3vvqU</li> </ol>"},{"location":"linear-algebra/solving-linear-equations/preface/","title":"Solving Linear Equations","text":""},{"location":"linear-algebra/solving-linear-equations/preface/#what","title":"What","text":"<p>A linear equation is an equation where all variables appear to the power 1 (no exponents, products, or functions like sin or log).</p> <p>A system of linear equations is a set of linear equations with the same variables.</p> <p>General form (for one equation in nn variables): \\(a_1x_1 + a_2x_2 + \\dots + a_nx_n = b\\)</p> <p>It looks like this: \\(ax+b=c\\)</p> <ul> <li>\\(x\\) is the unknown (what we want to find).</li> <li>\\(a,b,c\\) are numbers (called constants).</li> <li>The graph of a linear equation is always a straight line.</li> </ul>"},{"location":"linear-algebra/solving-linear-equations/preface/#how","title":"How","text":"<p>Number of Variables vs Number of Equations</p> <ul> <li>Let \\(n\\)  = number of variables</li> <li>Let \\(m\\) = number of equations</li> </ul> <p>Cases</p> <ol> <li> <p>Exactly determined</p> <p>If \\(m = n\\):</p> <ul> <li>The system might have a unique solution.</li> <li>Depends on whether equations are independent.</li> <li>A unique solution means the lines intersect at one point.</li> </ul> </li> <li> <p>Underdetermined</p> <p>If \\(m &lt; n\\):</p> <ul> <li>Fewer equations than unknowns</li> <li>Infinite solutions (if consistent) or none</li> <li>Example: 2 variables, 1 equation \u2192 line in 2D</li> </ul> </li> <li> <p>Overdetermined</p> <p>If \\(m &gt; n\\):</p> <ul> <li>More equations than unknowns</li> <li>May have no solution (inconsistent)</li> <li>If consistent, still can have a unique solution (e.g. in least squares problems)</li> </ul> </li> </ol>"},{"location":"linear-algebra/solving-linear-equations/preface/#why","title":"Why","text":"<p>Understanding the structure of linear systems is key to:</p> <ul> <li>Linear algebra</li> <li>Data science (e.g. regression)</li> <li>Physics (e.g. forces in equilibrium)</li> <li>Engineering (e.g. circuit analysis)</li> <li>Optimization</li> </ul> <p>Linear systems are the building blocks for more complex models in applied math and machine learning.</p>"},{"location":"ml/about/","title":"About","text":"<p>In progress...</p>"},{"location":"nlp/about/","title":"About","text":"<p>In progress...</p>"},{"location":"probability-theory/introduction/","title":"Introduction","text":""},{"location":"probability-theory/introduction/#what","title":"What","text":"<p>Probability is the mathematical framework used to quantify and model uncertainty in complex systems. It provides tools to describe the likelihood of various outcomes when the exact behavior of a system is too complicated or unknown.</p> <ul> <li>It assumes a known probability distribution that models how likely different outcomes are.</li> <li>Probability helps us say something quantifiable about future observations or events whose exact results are unknown.</li> <li>Examples include modeling measurement errors, gas molecules behavior, turbulence, weather, and human behavior.</li> </ul> <p>Key Concepts with Examples</p> <p>Uncertainty and Complex Systems</p> <ul> <li>Example: Cannot track every molecule in a gas, so we model temperature and entropy probabilistically using distributions like the Maxwell-Boltzmann distribution.</li> <li>Example: Turbulence in fluid flow is too complex to simulate exactly, so statistical models are used.</li> </ul> <p>Measurement Error</p> <ul> <li>Measurements (e.g., speed of light, gravity acceleration) have inherent errors, often modeled as normally distributed random variables. Repeating an experiment yields slightly different results each time.</li> <li>Often times measurement error behave according to Gaussian distribution (Genesis).</li> </ul> <p>Random Variables</p> <ul> <li>A variable (X) whose possible values have associated probabilities.</li> <li>Example: X could represent the number of heads in 10 coin flips.</li> <li>Distributions have parameters: e.g., Gaussian with mean (average) and standard deviation (spread).</li> </ul> <p>Counting Events</p> <ul> <li>Probability often reduces to counting how many outcomes satisfy certain criteria.</li> <li>Example: How many poker hands of a certain type are possible? What is the probability that three dice sum to 13?</li> </ul> <p>Note</p> <p>Probability vs. Statistics - Probability: Assume Known distribution, unknown future data; quantifies chance of events. - Statistics: Known data samples, unknown distribution; infer properties of the system from data. For example, ML for learning the probability distribution (i.e. parameters)</p> <p>Note</p> <p>Probability vs. Likelihood - Probability is the chance of observing a given outcome based on a known model. Likelihood is the chance of the model being true given a specific outcome. - Likelihood is a method/statistical concept used within statistics. In other words, probability asks \"What are the odds of seeing this data, given the model?\" while likelihood asks \"How likely is this model, given the data?\" - Probability is used when we know the model or process and want to predict how likely a particular outcome is. For example, if we know a coin is fair, the probability of getting heads is 50%. - Likelihood is used when we have data (the outcome) and want to figure out which model or parameter is most likely to have produced that data. For example, if we flip a coin and get heads 9 out of 10 times, we might use likelihood to determine how fair the coin is.  </p>"},{"location":"probability-theory/introduction/#how","title":"How","text":"<ul> <li>Model uncertainty by using probability distributions: For example, a normal (Gaussian) distribution models measurement errors. This means, to use parameters (e.g., mean and standard deviation) to define distributions.</li> <li>Use random variables to represent quantities that can take on different values with certain probabilities.</li> <li>Count possible outcomes and group events into sets to calculate probabilities.</li> <li>Apply probability to calculate the chance of events such as flipping 7 heads in 10 coin tosses or rolling dice sums.</li> <li>Use probability in dynamic systems by combining it with differential equations, leading to stochastic differential equations.</li> <li>Techniques like the Kalman filter merge control theory with probabilistic models to handle measurement uncertainty in real-time systems.</li> <li>Understand that some physical systems (coin flips, turbulence) are fundamentally deterministic but appear random due to complexity and measurement limits.</li> </ul>"},{"location":"probability-theory/introduction/#why","title":"Why","text":"<p>The real world is complex and uncertain, and it is often impossible to measure or model every detail exactly.  Probability offers an elegant way to simplify this complexity into manageable statistical quantities. It provides a framework for modeling measurement errors that naturally occur in experiments. Also, enables prediction and understanding of inherently unpredictable phenomena like weather and human behavior. It allows bridging from deterministic physics to practical modeling when full knowledge is unavailable.</p>"},{"location":"probability-theory/preface/","title":"Probability Theory","text":""},{"location":"probability-theory/preface/#books","title":"Books","text":"<ol> <li>\"First Course in Probability\" by Sheldon M. Ross</li> <li>\"Introduction to Probability\" by Dimitri Bertsekas and John Tsitsiklis</li> <li>\u201cIntroduction to Probability\u201d by Joseph K. Blitzstein, Jessica Hwang</li> </ol>"},{"location":"probability-theory/preface/#resources","title":"Resources","text":"Name URL Probability Bootcamp https://www.youtube.com/playlist?list=PLMrJAkhIeNNR3sNYvfgiKgcStwuPSts9V"},{"location":"probability-theory/axioms-of-probability/preface/","title":"Axioms of Probability","text":""},{"location":"probability-theory/axioms-of-probability/preface/#what","title":"What","text":"<p>The axioms of probability are the foundational rules that define valid probability measures in any probability space. There are three key axioms:</p> <ol> <li> <p>Axiom 1: Non-negativity</p> <p>\\(0 \\leq P(A) \\leq 1\\) for\u00a0any\u00a0event\u00a0\\(A\\).</p> <p>Defines: probability that the outcome of the experiment is an outcome in \\(A\\) is some number between 0 and 1.</p> </li> <li> <p>Axiom 2: Normalization (Total Probability)</p> <p>\\(P(S)=1\\) where\u00a0\\(S\\)\u00a0is\u00a0the\u00a0sample\u00a0space.</p> <p>Defines: with probability 1, the outcome will be a point in the sample space \\(S\\).</p> </li> <li> <p>Axiom 3: Countable Additivity (Disjoint Events)</p> <p>If\u00a0\\(A\u2229B=\u2205\\), then\u00a0\\(P(A \\cup B) = P(A) + P(B)\\)</p> <p>Defines: for any sequence of mutually exclusive events, the probability of at least one of these events occurring is just the sum of their respective probabilities.</p> <p>For any sequence of mutually exclusive events \\(E1, E2, ...\\)  (that is, events for which \\(E_iE_j= \u00d8\\) when \\(i \u2260 j\\)),</p> <p>$$ P\\left( \\bigcup_{i=1}^{\\infty} E_i \\right) = \\sum_{i=1}^{\\infty} P(E_i)</p> <p>$$</p> <p>We refer to \\(P(E)\\) as the probability of the event \\(E\\).</p> </li> </ol>"},{"location":"probability-theory/axioms-of-probability/preface/#how","title":"How","text":"<p>These axioms are applied to:</p> <ul> <li>Ensure probability values are valid (between 0 and 1)</li> <li>Build complex probabilities from simple events (e.g., unions, complements)</li> <li>Derive further rules like:<ul> <li>\\(P(A^c) = 1 - P(A)\\)<ul> <li>\\(A^c\\) or \\(A`\\) means everything but \\(A\\) inside sample space \\(S\\)</li> <li>\\(^c\\) or \\(`\\) is called as a complement in Set Theory.</li> </ul> </li> <li>\\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\)</li> <li>Inclusion\u2013exclusion principle</li> </ul> </li> </ul>"},{"location":"probability-theory/axioms-of-probability/preface/#why","title":"Why","text":"<ul> <li>Logical consistency: Forms the basis of probability theory</li> <li>Scalability: Allows building rules for combinations, sequences, and conditional events</li> <li>Universality: All valid probability models (discrete or continuous) obey these axioms</li> <li>Practicality: Used in statistics, machine learning, risk analysis, etc.</li> </ul>"},{"location":"probability-theory/axioms-of-probability/sample-space-and-events/","title":"Sample Space and Events","text":""},{"location":"probability-theory/axioms-of-probability/sample-space-and-events/#sample-space","title":"Sample Space","text":"<ul> <li>The sample space \\((\u03a9)\\) is the set of all possible outcomes of a random experiment.</li> <li>An element \\((\u03c9 \u2208 \u03a9)\\) represents a single outcome or realization.</li> <li>Examples of sample spaces:<ul> <li>All outcomes of flipping 5 coins</li> <li>Number of heads in 5 coin flips</li> <li>Daily high temperature</li> <li>Number of emails received in an hour</li> </ul> </li> </ul>"},{"location":"probability-theory/axioms-of-probability/sample-space-and-events/#events-and-subsets","title":"Events and Subsets","text":"<ul> <li>An event (A) is a subset of the sample space \u03a9.</li> <li>Example: For 3 coin flips:<ul> <li>\\(\u03a9\\) = {HHH, HHT, HTH, HTT, THH, THT, TTH, TTT}</li> <li>Event A: First flip is heads = {HHH, HHT, HTH, HTT}</li> <li>Event B: Second flip is tails = {HTH, HTT, TTH, TTT}</li> </ul> </li> </ul>"},{"location":"probability-theory/axioms-of-probability/sample-space-and-events/#set-operations-in-probability","title":"Set Operations in Probability","text":"<ul> <li>Union \\((A \u222a B)\\): Outcomes in A or B.<ul> <li>More inclusive.</li> </ul> </li> <li>Intersection \\((A \u2229 B)\\): Outcomes in both A and B.<ul> <li>More restrictive.</li> </ul> </li> <li>Complement \\((A\u1d9c)\\): Outcomes not in A.</li> </ul>"},{"location":"probability-theory/axioms-of-probability/sample-space-and-events/#venn-diagram-intuition","title":"Venn Diagram Intuition","text":"<ul> <li>Sample space \\(\u03a9\\) can be visualized as a rectangle.</li> <li>Events are subsets (circles) inside \\(\u03a9\\).</li> <li>Area of A / Area of \\(\u03a9 = P(A)\\) when outcomes are equally likely.</li> </ul>"},{"location":"probability-theory/axioms-of-probability/sample-space-and-events/#additional-derived-rules","title":"Additional Derived Rules","text":"<ul> <li>Complement Rule: \\(P(A\u1d9c) = 1 \u2212 P(A)\\)</li> <li>Empty Set: \\(P(\u2205) = 0\\)</li> <li>Subset Rule: If \\(A \u2286 B\\), then \\(P(A) \u2264 P(B)\\)</li> <li>Inclusion-Exclusion Principle: \\(P(A \u222a B) = P(A) + P(B) \u2212 P(A \u2229 B)\\)<ul> <li>We subtract the intersection to avoid counting the overlapping area twice.</li> <li>If the events are mutually exclusive this will be \\(P(A \u2229 B) = 0\\).</li> </ul> </li> </ul>"},{"location":"probability-theory/axioms-of-probability/sample-space-and-events/#event-types","title":"Event Types","text":""},{"location":"probability-theory/axioms-of-probability/sample-space-and-events/#certain-event","title":"Certain Event","text":"<ul> <li>\\(P(S) = 1\\)</li> <li>An event that always occurs.</li> <li>Denoted by the sample space SS.</li> </ul>"},{"location":"probability-theory/axioms-of-probability/sample-space-and-events/#independent-events","title":"Independent Events","text":"<ul> <li>Occurrence of one event does not affect the other.</li> <li>Probability rule: \\(P(A \\cap B) = P(A) \\cdot P(B)\\)</li> </ul>"},{"location":"probability-theory/axioms-of-probability/sample-space-and-events/#dependent-events","title":"Dependent Events","text":"<ul> <li>Occurrence of one event does affect the probability of the other.</li> <li>Probability rule: \\(P(A \\cap B) \\ne P(A) \\cdot P(B)\\)</li> </ul>"},{"location":"probability-theory/axioms-of-probability/sample-space-and-events/#mutually-exclusive-events","title":"Mutually Exclusive Events","text":"<ul> <li>\\(A \\cap B = \\emptyset\\)</li> <li>Two or more events cannot occur at the same time.</li> <li>Example: Drawing a card that is either a heart or a club (not both).</li> <li>Probability rule: \\(P(A \\cup B) = P(A) + P(B)\\)</li> <li>Mutually exclusive events are never independent (unless one has probability zero), because if \\(A \\cap B = 0\\), then \\(P(A \\cap B) \\ne P(A) \\cdot P(B)\\) (unless one of them is 0).</li> <li>Because mutually exclusive events cannot occur together, knowing that one event has happened means you know the other event definitely did not happen.</li> </ul>"},{"location":"probability-theory/axioms-of-probability/sample-space-and-events/#mutually-inclusive-events","title":"Mutually Inclusive Events","text":"<ul> <li>\\(A \\cap B \\ne \\emptyset\\)</li> <li>Two or more events can occur together (they may overlap).</li> <li>Example: Drawing a card that is a king or a heart (King of Hearts counts in both).</li> </ul>"},{"location":"probability-theory/axioms-of-probability/sample-space-and-events/#impossible-event","title":"Impossible Event","text":"<ul> <li>\\(P(\\emptyset) = 0\\)</li> <li>An event that can never occur.</li> <li>Represented by the empty set \\(\\emptyset\\).</li> </ul>"},{"location":"probability-theory/axioms-of-probability/sample-space-and-events/#complementary-events","title":"Complementary Events","text":"<ul> <li>\\(A \\cup A^c = S, A \\cap A^c = \\emptyset\\)</li> <li>Two events \\(A\\) and \\(A^c\\), such that exactly one must occur.</li> <li>Probability rule: \\(P(A^c) = 1 - P(A)\\)</li> </ul>"},{"location":"probability-theory/basic-principle-of-counting/combinations/","title":"Combinations","text":""},{"location":"probability-theory/basic-principle-of-counting/combinations/#definition","title":"Definition","text":"<p>We are often interested in determining how many different groups of \\(r\\) objects can be formed from a total of \\(n\\) distinct objects, when the order of selection does not matter.</p>"},{"location":"probability-theory/basic-principle-of-counting/combinations/#derivation","title":"Derivation","text":"<p>To count the number of such groups:</p> <ul> <li>If order mattered, we would have: \\(n(n - 1)(n - 2)\\cdots(n - r + 1) = \\frac{n!}{(n - r)!}\\)</li> <li> <p>But each group is counted \\(r!\\) times (all permutations of \\(r\\) items), so we divide:</p> \\[ \\text{Number of combinations} = \\frac{n!}{r!(n - r)!} \\] </li> </ul>"},{"location":"probability-theory/basic-principle-of-counting/combinations/#notation-and-terminology","title":"Notation and Terminology","text":"<p>The number of combinations is denoted as:</p> \\[ ^nC_r = \\binom{n}{r} = \\frac{n!}{r!(n - r)!} \\] <ul> <li>Where,<ul> <li>\\(n\\) = No. of total items</li> <li>\\(r\\) = No. of items chosen at once</li> <li>\\(0 \\leq r \\leq n\\)</li> </ul> </li> </ul>  \ud83d\udca1  $^nC_r$ read as \u201c**n choose r**\u201d and represents the number of ways to choose $r$ items from $n$ **without regard to order**.   <p>Special cases:</p> <ul> <li>\\(\\binom{n}{n} = \\binom{n}{0} = 1\\)</li> <li>\\(\\binom{n}{r} = 0\\) if \\(r &gt; n\\) or \\(r &lt; 0\\)</li> </ul>"},{"location":"probability-theory/basic-principle-of-counting/combinations/#example-basic-selection","title":"Example: Basic Selection","text":"<p>How many committees of 3 can be chosen from 20 people?</p> \\[ ^{20}C_3 = \\binom{20}{3} = \\frac{20 \\cdot 19 \\cdot 18}{3 \\cdot 2 \\cdot 1} = 1140 \\]"},{"location":"probability-theory/basic-principle-of-counting/combinations/#example-grouped-selection","title":"Example: Grouped Selection","text":"<p>How many committees of 2 women and 3 men from 5 women and 7 men?</p> \\[ \\binom{5}{2} \\cdot \\binom{7}{3} = 10 \\cdot 35 = 350 \\] <p>With constraint:</p> <p>If 2 men are feuding and cannot serve together, subtract combinations that include both:</p> <p>We select 1 men from remaining 5 men (because 2 men have already used the restricted space)  and 2 women from all the women.</p> \\[ \\text{Total Ways} - \\binom{5}{1} \\cdot \\binom{5}{2}\\Rightarrow 350 - 50 = 300 \\]"},{"location":"probability-theory/basic-principle-of-counting/combinations/#example-no-consecutive-defectives","title":"Example: No Consecutive Defectives","text":"<p>Given \\(n\\) antennas, \\(m\\) defective, and \\(n - m\\) functional, how many arrangements with no two defectives adjacent?</p> <ul> <li>Place \\(n - m\\) functional antennas: creates \\(n - m + 1\\) \u201cslots\u201d</li> <li>Choose \\(m\\) of those slots: \\(\\binom{n - m + 1}{m}\\)</li> </ul>"},{"location":"probability-theory/basic-principle-of-counting/combinations/#pascals-identity","title":"Pascal\u2019s Identity","text":"<p>Pascal's Identity, also known as Pascal's Rule, is\u00a0a fundamental combinatorial identity that relates binomial coefficients.\u00a0It states that for any positive integers n and k, with k between 1 and n-1, the binomial coefficient (n choose k) is equal to the sum of (n-1 choose k-1) and (n-1 choose k).\u00a0In simpler terms, each number in Pascal's Triangle is the sum of the two numbers directly above it.\u00a0</p> \\[ \\binom{n}{r} = \\binom{n - 1}{r - 1} + \\binom{n - 1}{r} \\] <p>Interpretation: Count of size-r groups that include a specific item + those that exclude it.</p> <p>Ts a fundamental identity in combinatorics that expresses a recursive relationship between binomial coefficients:</p>"},{"location":"probability-theory/basic-principle-of-counting/combinations/#binomial-theorem","title":"Binomial Theorem","text":"<p>The binomial theorem is primarily used\u00a0to expand expressions of the form \\((x + y)\u207f\\), where \\(n\\) is a non-negative integer.\u00a0It provides a formula to find the coefficients and terms in the expanded form of such expressions without direct multiplication.\u00a0Beyond basic algebra, it has applications in various fields like probability, statistics, and even disaster prediction.\u00a0</p> \\[ (x + y)^n = \\sum_{k = 0}^{n} \\binom{n}{k} x^k y^{n - k} \\] <p>Each term in the expansion corresponds to choosing \\(k\\) of the \\(n\\) terms to be \\(x\\), and the rest \\(y\\).</p>"},{"location":"probability-theory/basic-principle-of-counting/combinations/#example-expand-x-y3","title":"Example: Expand \\((x + y)^3\\)","text":"\\[ \\begin{gathered} (x + y)^3 = \\binom{3}{0}x^0y^3 + \\binom{3}{1}x^1y^2 + \\binom{3}{2}x^2y^1 + \\binom{3}{3}x^3y^0  \\\\[10pt] = y^3 + 3xy^2 + 3x^2y + x^3 \\end{gathered} \\]"},{"location":"probability-theory/basic-principle-of-counting/combinations/#example-total-number-of-subsets","title":"Example: Total Number of Subsets","text":"<p>How many subsets does a set of \\(n\\) elements have?</p> <p>Solution: \\(\\sum_{k = 0}^{n} \\binom{n}{k} = 2^n\\)</p> <p>Subsets with at least 1 element: \\(2^n - 1\\)</p>"},{"location":"probability-theory/basic-principle-of-counting/combinations/#example-counting-heads","title":"Example: Counting Heads","text":"<p>What is the probability of getting exactly 3 heads in 5 coin tosses?</p> <p>Solution: This is a binomial experiment</p> <ul> <li>Each toss has 2 outcomes: head (H) or tail (T).</li> <li>Each outcome is independent.</li> <li>Probability of head: p = 0.5, tail: 1\u2212p=0.5.</li> </ul> <p>We use the binomial probability formula:</p> <p>\\(P(k \\text{ heads in } n \\text{ tosses}) = \\binom{n}{k} p^k (1-p)^{n-k}\\)</p> <p>Here, \\(n = 5, k = 3, p = 0.5\\)</p> <p>Where,</p> <ul> <li>\\(p\\) = Probability of Success</li> <li>\\(q\\) = Probability of Failure \\((1-p)\\)</li> <li>\\(n\\) = No. of trials</li> <li>\\(k\\) = No. of times for a specific outcome within \\(n\\) trials</li> </ul> \\[ P(3 \\text{ heads}) = \\binom{5}{3} (0.5)^3 (0.5)^2 = \\binom{5}{3} (0.5)^5 = 10 \\times \\frac{1}{32} = \\frac{10}{32} = 0.3125 \\] <p>So, there is a 31.25% chance of getting exactly 3 heads in 5 fair coin tosses.</p>  \ud83d\udca1  **Binomial Theorem vs. Binomial Distribution**  - Binomial theorem is a formula for expanding powers of sums algebraically. - Binomial distribution applies this formula to model probabilities of successes in repeated independent trials."},{"location":"probability-theory/basic-principle-of-counting/combinations/#multinomial-theorem","title":"Multinomial Theorem","text":"<p>For expanding \\((x + y + z)^n\\) or \\((x_1 + x_2 + \\cdots + x_m)^n\\), the multinomial theorem is used.</p> <p>It generalizes the binomial theorem to more than two terms. The expansion is:</p> \\[ (x + y + z)^n = \\sum_{a+b+c = n} \\frac{n!}{a! \\, b! \\, c!} \\, x^a y^b z^c \\] <p>where \\(a, b, c \\geq 0\\) and \\(a + b + c = n\\).</p> <p>The coefficients \\(\\frac{n!}{a! b! c!}\\) are called multinomial coefficients.</p> <p>The general form is:</p> \\[ (x_1 + x_2 + \\cdots + x_m)^n = \\sum_{a_1 + a_2 + \\cdots + a_m = n} \\frac{n!}{a_1! a_2! \\cdots a_m!} \\, x_1^{a_1} x_2^{a_2} \\cdots x_m^{a_m} \\] <p>where each \\(a_i \\geq 0\\) and the sum of all \\(a_i\\) equals \\(n\\).</p>"},{"location":"probability-theory/basic-principle-of-counting/combinations/#summary-of-binomial-coefficients","title":"Summary of Binomial Coefficients","text":"\\[ \\begin{aligned} &amp;^nC_0 = \\binom{n}{0} = 1 \\\\ &amp;^nC_n = \\binom{n}{n} = 1 \\\\ &amp;^nC_1 = \\binom{n}{1} = n \\\\ &amp;^nC_r = \\binom{n}{r} = \\binom{n}{n-r} \\quad \\text{(symmetry)} \\\\ &amp;^nC_r = \\binom{n}{r} = \\frac{n!}{r!(n-r)!} \\\\ &amp;0! = 1 \\\\ &amp;^nC_r = \\binom{n}{r} = \\binom{n-1}{r-1} + \\binom{n-1}{r} \\quad \\text{(Pascal's rule)} \\\\ &amp;^nC_r = \\binom{n}{r} = 0 \\quad \\text{if } r &gt; n \\text{ or } r &lt; 0 \\end{aligned} \\]"},{"location":"probability-theory/basic-principle-of-counting/permutations/","title":"Permutations","text":""},{"location":"probability-theory/basic-principle-of-counting/permutations/#definition","title":"Definition","text":"<p>A permutation is an ordered arrangement of objects. The number of permutations of \\(n\\) distinct objects is: </p> \\[ n! = n \\times (n - 1) \\times (n - 2) \\times \\ldots \\times 3 \\times 2 \\times 1 \\] <p>By definition, \\(0! = 1\\).</p>"},{"location":"probability-theory/basic-principle-of-counting/permutations/#example-1-arranging-letters","title":"Example 1: Arranging Letters","text":"<p>Question: How many different ordered arrangements of the letters a, b, and c are possible (without repetition)?</p> <p>Solution: Each position can be filled as follows:</p> <ul> <li>3 choices for the first letter</li> <li>2 choices for the second</li> <li>1 choice for the last</li> </ul> <p>\\(3 \\times 2 \\times 1 = 6 \\text{ permutations: } abc, acb, bac, bca, cab, cba\\)</p>"},{"location":"probability-theory/basic-principle-of-counting/permutations/#example-2-ranking-students","title":"Example 2: Ranking Students","text":"<p>Question: A class has 6 men and 4 women. If all 10 students are ranked uniquely, how many different rankings are possible?</p> <p>Solution: \\(10! = 3,628,800\\) total rankings.</p> <p>If men are ranked among themselves and women among themselves: \\(6! \\times 4! = 720 \\times 24 = 17,280\\) rankings.</p>"},{"location":"probability-theory/basic-principle-of-counting/permutations/#ordering-and-replacement","title":"Ordering and Replacement","text":"<p>Permutations arise when order matters in counting outcomes.</p>"},{"location":"probability-theory/basic-principle-of-counting/permutations/#order-matters","title":"Order Matters","text":"<ul> <li> <p>Example 1 (order matters): Flipping a Coin</p> <ul> <li>Flipping a coin 10 times \u2014 number of unique sequences = \\(2^{10}\\) because each flip has 2 outcomes and flips are independent. So, if we create a binary tree, total levels would be 10 and at the last level total leaves would be 1024.</li> </ul> <pre><code>Level 0:       Start\n               /    \\\nLevel 1:     H       T\n            / \\     / \\\nLevel 2:  H   T   H   T\n          ...      ...\n</code></pre> <ul> <li>Order matters here because sequence HT is different from TH.</li> <li>General formula for permutations (order matters, with replacement):</li> </ul> </li> </ul> \\[ \\text{No. of Sequences (\\#)} = n^r \\] <ul> <li>Example 2 (order matters, without replacement): Poker<ul> <li>Dealing 5 cards from a 52-card deck where order matters.</li> <li>Number of ordered 5-card sequences = \\(52 \\times 51 \\times 50 \\times 49 \\times 48\\).</li> <li>This is called sampling without replacement since cards are not put back in the deck.</li> <li>This can be expressed using factorials: \\(\\frac{52!}{(52 - 5)!} = \\frac{52!}{47!}\\)</li> </ul> </li> <li> <p>General formula for permutations (order matters, without replacement):</p> \\[ P(n, r) = \\frac{n!}{(n-r)!} \\] <p>where</p> <p>\\(n\\) = total number of items to choose from (choices)</p> <p>\\(r\\) = number of items chosen (sampling)</p> </li> </ul>"},{"location":"probability-theory/basic-principle-of-counting/permutations/#order-does-not-matters","title":"Order Does Not Matters","text":"<p>When order doesn\u2019t matter, you divide permutations by \\(r!\\) because each combination can be permuted \\(r!\\) ways which count as the same set. </p> <p>General formula for permutations (order doesn\u2019t matters, without replacement):</p> \\[ \\frac{n!}{(n-r)! r!} \\] <p>Example of Poker from above:</p> <ul> <li>Dealing 5 cards from a 52-card deck where order doesn\u2019t matters.</li> <li>Number of ordered 5-card sequences = \\(52 \\times 51 \\times 50 \\times 49 \\times 48\\).</li> <li>A,K,Q,J,10 or 10,J,Q,K,A \u2013 anything will work.</li> <li>There are \\(r!\\) ways of permuting of above 5 cards uniquely. All the orders are equivalent.</li> <li>This can be expressed using factorials: \\(\\frac{52!}{(52 - 5)! 5!} = \\frac{52!}{47! 5!}\\)</li> </ul> <p>The permutation formula counts unique ordered sequences, the combination formula counts unique unordered sets.</p>"},{"location":"probability-theory/basic-principle-of-counting/preface/","title":"Basic Principle of Counting","text":"<p>Probability helps us determine how likely an event is to happen. To put simply, probability is counting the number of ways an event can happen divided by the number of total possible outcomes that can happen.</p> \\[ \\text{Probability of Event A} = \\frac{\\text{Number of outcomes where A occurs}}{\\text{Total number of possible outcomes}} \\]"},{"location":"probability-theory/basic-principle-of-counting/preface/#the-basic-principle-of-counting","title":"The Basic Principle of Counting","text":"<p>If you are doing two things one after the other, and the first thing can happen in m ways, and for each of those, the second thing can happen in n ways, then the total number of possible outcomes is m \u00d7 n.</p> <p>Why this is true:</p> <p>You can list out every possible combination. For each of the m options from the first thing, there are n options from the second thing. So you end up with m rows, and each row has n items. This means there are m \u00d7 n total combinations.</p>"},{"location":"probability-theory/basic-principle-of-counting/preface/#simple-examples","title":"Simple Examples","text":""},{"location":"probability-theory/basic-principle-of-counting/preface/#example-1-flipping-two-coins","title":"Example 1: Flipping Two Coins","text":"<p>Question: What is the probability of getting at least one heads?</p> <ul> <li>Sample space \\((\u03a9)\\): \\({\\{HH, HT, TH, TT\\}}\\) \u2192 4 outcomes</li> <li>Event A: at least one heads \u2192 {HH, HT, TH} \u2192 3 outcomes \u2192 \\(P(A) = \\frac{3}{4} = 0.75\\)</li> </ul> <p>Question: What is the probability of no heads?</p> <ul> <li>Event B: no heads \u2192 \\({TT}\\) \u2192 1 outcome \u2192 \\((B) = \\frac{1}{4} = 0.25\\)</li> </ul>"},{"location":"probability-theory/basic-principle-of-counting/preface/#example-2-rolling-two-dice","title":"Example 2: Rolling Two Dice","text":"<p>Question: What is the probability that at least one die is a 5?</p> <ul> <li>Total possible outcomes: 6 \u00d7 6 = 36</li> <li>Favorable outcomes:<ul> <li>First die is 5 \u2192 (5,1) to (5,6): 6 outcomes</li> <li>Second die is 5 but first is not \u2192 (1,5), (2,5), (3,5), (4,5), (6,5): 5 outcomes</li> <li>Total favorable = 6 + 5 = 11</li> <li>Probability: \\(P(A) = \\frac{11}{36}\\)</li> </ul> </li> </ul>"},{"location":"probability-theory/basic-principle-of-counting/preface/#example-3-choosing-samples","title":"Example 3: Choosing Samples","text":"<p>Question: There are 10 women. Each woman has 3 children. You want to pick one woman and one of her children for best mother-child pair award.</p> <ul> <li>First, you have 10 choices for the woman (m options).</li> <li>Then, for each woman, you have 3 choices (her children) (n options).</li> <li>So the total number of different choices is: 10 \u00d7 3 = 30 choices.</li> </ul>"},{"location":"probability-theory/basic-principle-of-counting/preface/#counting-as-the-foundation","title":"Counting as the Foundation","text":"<p>When events become more complex (e.g., flipping 1000 coins, or rolling 15 dice), brute-force enumeration becomes impractical. This is where probability theory evolves into using:</p> <ul> <li>Combinatorics (permutations and combinations)</li> <li>Formulas and models (e.g., binomial theorem, distributions)</li> <li>Set theory and logic (to define events clearly)</li> </ul> <p>Key Assumptions (in Basic Models)</p> <ol> <li>Uniform Probability: All outcomes are equally likely (fair coin, fair die).</li> <li>Independence: Events do not affect each other (each flip or roll is independent).</li> </ol> <p>What Is Random?</p> <p>A process is \u201crandom\u201d if we cannot predict its outcome with the information available.</p> <ul> <li>Deterministic but unknown: A coin flip is physically deterministic, but too complex to model precisely\u2014so we treat it as random.</li> <li>Truly stochastic: Radioactive decay is genuinely random (quantum uncertainty).</li> </ul>"},{"location":"probability-theory/basic-principle-of-counting/preface/#applications-of-counting-probability","title":"Applications of Counting &amp; Probability","text":"<ul> <li>Games (poker, dice, darts)</li> <li>Weather forecasts</li> <li>Medical diagnostics</li> <li>Failure modeling in engineering</li> <li>Financial risk assessment</li> <li>Machine learning and AI</li> </ul>"},{"location":"probability-theory/conditional-probability/bayes-theorem/","title":"Bayes Theorem","text":""},{"location":"probability-theory/conditional-probability/bayes-theorem/#introduction-and-intuition","title":"Introduction and Intuition","text":"<p>Bayes' Theorem is one of the most powerful and widely-used tools in probability and statistics. It allows us to update our beliefs about an event based on new evidence, and is essential in:</p> <ul> <li>Statistics</li> <li>Machine Learning (especially Bayesian models)</li> <li>Medical diagnosis</li> <li>Inverse problems in engineering and science</li> </ul>"},{"location":"probability-theory/conditional-probability/bayes-theorem/#conditional-probability-refresher","title":"Conditional Probability Refresher","text":"<p>We define the probability of event A given that event B has occurred as:</p> \\[ P(A|B) = \\frac{P(A \\cap B)}{P(B)} \\] <p>This represents an update to the probability of A when we know that B has happened.</p>"},{"location":"probability-theory/conditional-probability/bayes-theorem/#multiplication-rule","title":"Multiplication Rule","text":"<p>From the definition above, we also get: \\(P(A \\cap B) = P(A|B) \\cdot P(B)\\)</p> <p>This is known as the multiplication rule, and it can also be written as: \\(P(B \\cap A) = P(B|A) \\cdot P(A)\\)</p> <p>This symmetry is important and is used to derive Bayes' Theorem.</p>"},{"location":"probability-theory/conditional-probability/bayes-theorem/#why-do-we-want-to-reverse-the-conditioning","title":"Why Do We Want to Reverse the Conditioning?","text":"<p>Often, we know:</p> <ul> <li>\\(P(\\text{symptom} \\mid \\text{disease})\\): how likely the symptom is if someone has the disease.</li> </ul> <p>But what we want is:</p> <ul> <li>\\(P(\\text{disease} \\mid \\text{symptom})\\): how likely someone has the disease given the symptom (this is the inverse problem).</li> </ul> <p>This reversal is essential in diagnostics, prediction, and decision making.</p>"},{"location":"probability-theory/conditional-probability/bayes-theorem/#bayes-theorem-basic-form","title":"Bayes\u2019 Theorem (Basic Form)","text":"\\[ P(B|A) = \\frac{P(A|B) \\cdot P(B)}{P(A)} \\] <p>This is derived by noting:</p> <ul> <li>\\(P(A \\cap B) = P(A|B) \\cdot P(B) = P(B|A) \\cdot P(A)\\)</li> </ul>"},{"location":"probability-theory/conditional-probability/bayes-theorem/#bayesian-terminology","title":"Bayesian Terminology","text":"<ul> <li>Posterior \\(P(B|A)\\): updated belief after seeing evidence A.</li> <li>Likelihood (Update) \\(P(A|B)\\): probability of seeing evidence A if B is true.</li> <li>Prior \\(P(B)\\): belief about B before seeing evidence.</li> <li>Evidence (Marginal) \\(P(A)\\): total probability of observing A (normalizing constant).</li> </ul> <p>Bayes\u2019 Theorem expresses: \\(\\text{Posterior} = \\frac{\\text{Likelihood} \\times \\text{Prior}}{\\text{Evidence}}\\)</p>"},{"location":"probability-theory/conditional-probability/bayes-theorem/#bayes-theorem-extended-form-law-of-total-probability","title":"Bayes\u2019 Theorem (Extended Form: Law of Total Probability)","text":"<p>Often, \\(P(A)\\) is hard to compute directly. But we can break it into components using the law of total probability:</p> \\[ P(A) = P(A|B) \\cdot P(B) + P(A|B^c) \\cdot P(B^c) \\] <p>So the full expression becomes:</p> \\[ P(B|A) = \\frac{P(A|B) \\cdot P(B)}{P(A|B) \\cdot P(B) + P(A|B^c) \\cdot P(B^c)} \\] <p>This is extremely useful when we only know conditional probabilities and priors.</p>"},{"location":"probability-theory/conditional-probability/bayes-theorem/#bayes-theorem-general-form-for-partitioned-sample-space","title":"Bayes\u2019 Theorem (General Form for Partitioned Sample Space)","text":"<p>If events \\(B_1, B_2, ..., B_n\\) form a partition of the sample space (disjoint and exhaustive), then:</p> \\[ P(B_j|A) = \\frac{P(A|B_j) \\cdot P(B_j)}{\\sum_{i=1}^n P(A|B_i) \\cdot P(B_i)} \\] <p>This is useful in multiclass classification and hypothesis testing with multiple possible causes.</p>"},{"location":"probability-theory/conditional-probability/bayes-theorem/#example-cancer-screening-paradox","title":"Example: Cancer Screening Paradox","text":""},{"location":"probability-theory/conditional-probability/bayes-theorem/#setup","title":"Setup:","text":"<ul> <li>Test is 99% accurate.<ul> <li>\\(P(\\text{+}|\\text{disease}) = 0.99\\)</li> <li>\\(P(\\text{+}|\\text{no disease}) = 0.01\\)</li> </ul> </li> <li>Disease is rare:<ul> <li>\\(P(\\text{disease}) = 0.001\\)<ul> <li>i.e. only 1 in 1000 people have the disease</li> </ul> </li> <li>\\(P(\\text{no disease}) = 0.999\\)</li> </ul> </li> </ul> <p>We want: \\(P(\\text{disease}|\\text{+})\\)</p>"},{"location":"probability-theory/conditional-probability/bayes-theorem/#apply-bayes-theorem","title":"Apply Bayes\u2019 Theorem:","text":"\\[ P(\\text{disease}|\\text{+}) = \\frac{P(\\text{+}|\\text{disease}) \\cdot P(\\text{disease})}{P(\\text{+}|\\text{disease}) \\cdot P(\\text{disease}) + P(\\text{+}|\\text{no disease}) \\cdot P(\\text{no disease})} \\] \\[ =\\frac{0.99 \\cdot 0.001}{0.99 \\cdot 0.001 + 0.01 \\cdot 0.999} \\approx \\frac{0.00099}{0.00099 + 0.00999} = \\frac{0.00099}{0.01098} \\approx 0.09 \\]"},{"location":"probability-theory/conditional-probability/bayes-theorem/#interpretation","title":"Interpretation:","text":"<p>Even with a 99% accurate test, the chance you actually have cancer given a positive result is only 9%.</p> <p>Why? Because the disease is so rare, the false positives dominate.</p> <p>This is why positive results in screening often lead to secondary confirmatory tests.</p>"},{"location":"probability-theory/conditional-probability/bayes-theorem/#sequential-updates-and-priors","title":"Sequential Updates and Priors","text":"<p>Bayesian reasoning supports sequential data updates:</p> <ol> <li>Start with a prior (e.g., the coin is fair).</li> <li>Gather data (e.g., coin shows tails 10 times).</li> <li>Compute the posterior (updated belief).</li> <li>Use that posterior as the new prior for the next observation.</li> <li>Repeat.</li> </ol> <p>This principle is at the heart of Bayesian machine learning, e.g., in:</p> <ul> <li>Bayesian optimization</li> <li>Bayesian networks</li> <li>Bayesian deep learning</li> </ul> <p>Bayesian thinking is about updating beliefs with new evidence. In practice, prior knowledge and base rates dramatically affect interpretation, especially for rare events. Think carefully about what you can measure (e.g., test results) vs. what you really want to know (e.g., actual condition).</p>"},{"location":"probability-theory/conditional-probability/independence/","title":"Independence","text":""},{"location":"probability-theory/conditional-probability/independence/#statistical-independence","title":"Statistical Independence","text":"<p>Definition:</p> <p>Two events \\(A\\) and \\(B\\) are independent if knowing that one of them occurred gives no additional information about the probability of the other occurring.</p> <ul> <li>Formally: \\(P(A \\mid B) = P(A) \\quad \\text{and} \\quad P(B \\mid A) = P(B)\\)</li> </ul> <p>This means that the occurrence of one event does not affect the probability of the other.</p>"},{"location":"probability-theory/conditional-probability/independence/#key-consequence-of-independence","title":"Key Consequence of Independence","text":"<p>If \\(A\\) and \\(B\\) are independent: </p> \\[ P(A \\cap B) = P(A) \\cdot P(B) \\] <p>Proof using conditional probability:</p> <p>We know:</p> \\[ P(A \\mid B) = \\frac{P(A \\cap B)}{P(B)} \\] <p>If \\(A\\) and \\(B\\) are independent, then \\(P(A \\mid B) = P(A)\\), so:</p> \\[ P(A) = \\frac{P(A \\cap B)}{P(B)} \\Rightarrow P(A \\cap B) = P(A) \\cdot P(B) \\]"},{"location":"probability-theory/conditional-probability/independence/#example-deck-of-cards","title":"Example: Deck of Cards","text":"<p>For a single card draw:</p> <ul> <li>Let \\(A\\): card is a Spade \u2192 \\(P(A) = \\frac{13}{52} = \\frac{1}{4}\\)</li> <li>Let \\(B\\): card is a Queen \u2192 \\(P(B) = \\frac{4}{52} = \\frac{1}{13}\\)</li> </ul> <p>Then:</p> <ul> <li>\\(A \\cap B\\): card is the Queen of Spades \u2192 \\(P(A \\cap B) = \\frac{1}{52}\\)</li> <li>Since  \\(\\frac{1}{4} \\cdot \\frac{1}{13} = \\frac{1}{52}\\), \\(A\\) and \\(B\\) are independent.</li> </ul> <p>Selecting either of the cards won\u2019t affect the probability of the other one. </p> <p>Graphically, you can imagine the full deck as a grid of suits and ranks. The Spades form 1/4 of the set, and the Queens form 1/13. Their intersection, the Queen of Spades is just one card out of 52.</p>"},{"location":"probability-theory/conditional-probability/independence/#example-non-independent-event","title":"Example: Non-Independent Event","text":"<p>For a single card draw:</p> <ul> <li>\\(A\\): selecting a black card \u2192 26 black cards (Spades + Clubs)<ul> <li>\\(P(A) = \\frac{26}{52} = \\frac{1}{2}\\)</li> </ul> </li> <li>\\(B\\): selecting a spade \u2192 13 cards<ul> <li>\\(P(B) = \\frac{13}{52} = \\frac{1}{4}\\)</li> </ul> </li> <li>\\(P(A \\cap B) = P(\\text{card is a spade}) = \\frac{13}{52} = \\frac{1}{4}\\)</li> </ul> <p>Since \\(P(A) \\cdot P(B) = \\frac{1}{8} \\neq \\frac{1}{4} = P(A \\cap B)\\), the events are dependent. This makes intuitive sense: if you know a card is black, you've increased the probability it's a spade from 1/4 to 1/2 (since spades are half of all black cards).</p>"},{"location":"probability-theory/conditional-probability/independence/#independence-in-practice-reliability","title":"Independence in Practice: Reliability","text":""},{"location":"probability-theory/conditional-probability/independence/#series-configuration","title":"Series Configuration","text":"<ul> <li>n components in series (e.g., Christmas lights):<ul> <li>If any component fails, the system fails.</li> <li>Let failure probability of one component be pp</li> <li>Probability of success: \\(P(\\text{success}) = (1 - p)^n\\)</li> <li>Therefore, \\(P(\\text{failure}) = 1 - (1 - p)^n\\)</li> </ul> </li> </ul> <p>Example:</p> <p>If \\(p = 0.05\\), \\(n = 10\\):</p> <p>\\(P(\\text{failure}) = 1 - (0.95)^{10} \\approx 0.40\\)</p> <p>So, even components with 95% reliability fail 40% of the time in series.</p>"},{"location":"probability-theory/conditional-probability/independence/#parallel-configuration","title":"Parallel Configuration","text":"<ul> <li>n components in parallel:<ul> <li>System fails only if all components fail.</li> <li>Probability of failure: \\(P(\\text{failure}) = p^n\\)</li> </ul> </li> </ul> <p>Example:</p> <p>If \\(p = 0.05\\), \\(n = 10\\):</p> <p>\\(P(\\text{failure}) = (0.05)^{10} \\approx 10^{-13}\\)</p> <p>So, parallel configurations are vastly more reliable under the assumption of independence.</p>"},{"location":"probability-theory/conditional-probability/introduction/","title":"Introduction","text":""},{"location":"probability-theory/conditional-probability/introduction/#conditional-probability-core-concept","title":"Conditional Probability: Core Concept","text":"<ul> <li>Conditional probability allows you to update the probability of an event A given that another event B has occurred.</li> <li>It answers: How does knowing that B happened affect the likelihood of A?</li> </ul>"},{"location":"probability-theory/conditional-probability/introduction/#notation-and-definition","title":"Notation and Definition","text":"<ul> <li>Written as: \\(P(A \\mid B) = \\frac{P(A \\cap B)}{P(B)}\\)</li> <li>This is the probability that both A and B happen, divided by the probability that B happens.</li> <li>Conceptually: zoom into the space where B is true and ask how likely A is inside that space.</li> </ul>"},{"location":"probability-theory/conditional-probability/introduction/#visual-interpretation","title":"Visual Interpretation","text":"<ul> <li>Imagine the total probability space as a big set \u03a9.</li> <li>Event A is a subset of \u03a9; event B is another subset.</li> <li>When B happens, we restrict our view to only the region B.</li> <li>Then P(A\u2223B)P(A \\mid B) is the ratio of the overlapping part of A and B to the total area of B.</li> </ul>"},{"location":"probability-theory/conditional-probability/introduction/#key-insight","title":"Key Insight","text":"<ul> <li>Learning B changes your belief about A.</li> <li>In some cases, it does not change (events are independent), but often it does.</li> <li>Conditional probability is a way to refine or update your expectations based on partial information.</li> </ul>"},{"location":"probability-theory/conditional-probability/introduction/#examples","title":"Examples","text":"<ol> <li>Dice Rolls (Independent Events)<ul> <li>A: first die is 3</li> <li>B: second die is 5</li> <li>\\(P(A \\mid B) = P(A) = \\frac{1}{6}\\) \u2192 B tells you nothing about A.</li> </ul> </li> <li>Dice and Sum (Dependent Events)<ul> <li>A: first die is 3 \u2192 \\(P(A) = 6/36 = 1/6\\)</li> <li>C: sum of both dice is 6 \u2192 \\(P(C) = 5/36\\)</li> <li>Knowing the total is 6 does affect the chance the first die is 3. You have fewer valid (die1, die2) pairs to consider.</li> <li>Possible outcomes where sum is 6:<ul> <li>(1,5), (2,4), (3,3), (4,2), (5,1) \u2192 5 outcomes</li> <li>Only (3,3) has first die = 3 \u2192 1 outcome</li> <li>\\(P(A \\mid C) = \\frac{1}{5}\\)</li> </ul> </li> </ul> </li> <li> <p>Cards (Partial Information Impact)</p> <ul> <li>A: card is a spade (4 suits): \\(P(A) = \\frac{1}{4}\\)</li> <li>B: card is black (2 reds, 2 blacks): \\(P(B) = \\frac{2}{4}= \\frac{1}{2}\\)</li> <li> <p>Since spades are black, \\(A \\subseteq B\\), so knowing the card is black narrows it to spade or club.</p> <p>\\(P(A \\mid B) = \\frac{P(A \\cap B)}{P(B)} = \\frac{\\frac{1}{4}}{\\frac{1}{2}} = \\frac{1}{2}\\)</p> </li> <li> <p>If card is red \u2192 \\(P(\\text{spade} \\mid \\text{red}) = 0\\)</p> </li> </ul> </li> </ol>"},{"location":"probability-theory/conditional-probability/introduction/#real-world-application-medical-testing-inverse-problems","title":"Real-World Application: Medical Testing (Inverse Problems)","text":"<ul> <li>A: test result is positive</li> <li>B: person has cancer</li> <li>\\(P(A \\mid B) = 90\\%\\) \u2013 test catches 90% of real cases</li> <li>But what we really want: \\(P(B \\mid A)\\): the chance someone has cancer given a positive test \u2192 this is not 90% due to false positives.</li> <li>This is an inverse problem: estimating a hidden cause (B) from an observed effect (A).</li> <li>Solving these inverse problems requires Bayes Theorem.</li> </ul>"},{"location":"probability-theory/conditional-probability/introduction/#why-conditional-probability-matters","title":"Why Conditional Probability Matters","text":"<ul> <li>It enables inference: making judgments from partial information.</li> <li>Crucial for machine learning, diagnostic tests, recommendation systems, and risk analysis.</li> <li>Foundation for Bayesian reasoning, which we use to update beliefs under uncertainty.</li> </ul>"},{"location":"probability-theory/conditional-probability/law-of-total-probability/","title":"Law of Total Probability","text":""},{"location":"probability-theory/conditional-probability/law-of-total-probability/#multiplication-law","title":"Multiplication Law","text":"<p>An important outcome of conditional probability is the multiplication law. The probability of both A and B happening is equal to the probability of A given B multiplied by the probability of B. Formally:</p> \\[ P(A \\cap B) = P(A|B) \\times P(B) \\] <p>This is trivial but very useful, and we will use it all the time.</p> <p>The multiplication rule expresses the probability of multiple events all occurring together. For events \\(E1,E2,\u2026,EnE_1, E_2, \\ldots, E_n\\), the rule states:</p> \\[ P(E_1 E_2 E_3 \\cdots E_n) = P(E_1) \\times P(E_2 | E_1) \\times P(E_3 | E_1 E_2) \\times \\cdots \\times P(E_n | E_1 E_2 \\cdots E_{n-1}) \\] <p>In other words, the probability that all events \\(E_1, E_2, \\ldots, E_n\\) happen is found by multiplying:</p> <ul> <li>The probability that \\(E_1\\) occurs,</li> <li>The probability that \\(E_2\\) occurs given \\(E_1\\) has occurred,</li> <li>The probability that \\(E_3\\) occurs given both \\(E_1\\) and \\(E_2\\) have occurred,</li> <li>And so forth, up to the probability that \\(E_n\\) occurs given all previous events \\(E_1, \\ldots, E_{n-1}\\).</li> </ul> <p>Proof Sketch</p> <p>To prove this, start with the right-hand side and apply the definition of conditional probability repeatedly:</p> \\[ P(E_1) \\times \\frac{P(E_1 E_2)}{P(E_1)} \\times \\frac{P(E_1 E_2 E_3)}{P(E_1 E_2)} \\times \\cdots \\times \\frac{P(E_1 E_2 \\cdots E_n)}{P(E_1 E_2 \\cdots E_{n-1})} = P(E_1 E_2 \\cdots E_n) \\] <p>In this product, all intermediate terms cancel out, leaving: \\(P(E_1 E_2 \\cdots E_n)\\) which is the probability that all events occur simultaneously.</p>"},{"location":"probability-theory/conditional-probability/law-of-total-probability/#the-law-of-total-probability","title":"The Law of Total Probability","text":"<p>The law of total probability is another key concept. Suppose you break your sample space \\(\\Omega\\) into disjoint sets \\(B_1, B_2, \\ldots, B_n\\) such that:</p> \\[ \\Omega = \\bigcup_{i=1}^n B_i \\] <p>and the sets are disjoint, meaning no overlap between any \\(B_i\\) and \\(B_j\\) if \\(i \\neq j\\).</p> <p>The law of total probability states that for any event A:</p> \\[ P(A) = \\sum_{i=1}^n P(A|B_i) \\times P(B_i) \\] <p>This means you can compute the probability of A by summing the conditional probabilities of A given each disjoint set BiB_i, weighted by the probabilities of those sets.</p>"},{"location":"probability-theory/conditional-probability/law-of-total-probability/#example-card-suits","title":"Example: Card Suits","text":"<p>Imagine a deck of 52 cards divided into four disjoint sets: hearts, diamonds, clubs, and spades (each with probability 1/4). Let event A be drawing a red card (hearts or diamonds). Using the law of total probability:</p> \\[ P(A) = P(A|B_1)P(B_1) + P(A|B_2)P(B_2) + P(A|B_3)P(B_3) + P(A|B_4)P(B_4) \\] <p>where:</p> <ul> <li>\\(B_1\\) = hearts (red),</li> <li>\\(B_2\\) = diamonds (red),</li> <li>\\(B_3\\) = clubs (not red),</li> <li>\\(B_4\\) = spades (not red).</li> </ul> <p>Calculating,</p> \\[ P(A) = 1 \\times \\frac{1}{4} + 1 \\times \\frac{1}{4} + 0 \\times \\frac{1}{4} + 0 \\times \\frac{1}{4} = \\frac{1}{2} \\]"},{"location":"probability-theory/conditional-probability/law-of-total-probability/#disjoint-sets-and-complements","title":"Disjoint Sets and Complements","text":"<p>A very useful special case is when the sample space is divided into an event B and its complement \\(B^c\\). The law of total probability becomes:</p> \\[ P(A) = P(A|B)P(B) + P(A|B^c)P(B^c) \\] <p>This expresses the total probability of A happening as a weighted sum over whether B happens or not.</p>"},{"location":"probability-theory/conditional-probability/preface/","title":"Conditional Probability","text":""},{"location":"probability-theory/conditional-probability/preface/#what","title":"What","text":"<p>Conditional Probability is the probability of an event A occurring given that another event B has already occurred.</p> <p>It is denoted as: \\(P(A \\mid B)\\)</p> <p>This expresses how the likelihood of A changes when we know B has happened.</p>"},{"location":"probability-theory/conditional-probability/preface/#how","title":"How","text":"<p>The conditional probability is calculated using the formula: </p> <p>If \\(P(B) &gt; 0\\), then</p> \\[ P(A \\mid B) = \\frac{P(A \\cap B)}{P(B)} \\] <p>Where:</p> <ul> <li>\\(P(A \\cap B)\\) is the probability that both A and B occur.</li> <li>\\(P(B)\\) is the probability that B occurs (must be &gt; 0).</li> </ul>"},{"location":"probability-theory/conditional-probability/preface/#example","title":"Example:","text":"<p>If flipping two coins:</p> <ul> <li>Let A = first flip is heads</li> <li> <p>Let B = both flips are heads</p> <p>Then,</p> </li> </ul> \\[ P(A \\mid B) = \\frac{P(\\text{first = H and both = HH})}{P(\\text{both = HH})} = \\frac{P(HH)}{P(HH)} = 1 \\]"},{"location":"probability-theory/conditional-probability/preface/#why","title":"Why","text":"<p>Conditional probability is essential because:</p> <ul> <li>It allows updating beliefs when new information is known (Bayesian reasoning).</li> <li>It helps model dependent events (where the outcome of one event affects another).</li> <li>It is foundational for concepts like Bayes\u2019 Theorem, Markov Chains, and Machine Learning models.</li> </ul> <p>It moves probability from static, one-time events to dynamic, context-aware inference.</p>"},{"location":"rl/about/","title":"About","text":"<p>In progress...</p>"}]}